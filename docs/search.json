[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "“The unexpected elation with which I had talked about mathematics had suddenly evaporated, and I sat beside him, feeling the weight of my own body, its unnecessary size. Outside of mathematics we had nothing to say to each other, and we both knew it. Then it occurred to me that the emotion with which I had spoken of the blessed role of mathematics on the voyage was a deception. I had been deceiving myself with the modesty, the serious heroism of the pilot who occupies himself, in the gaps of the nebulae, with theoretical studies of infinity. Hypocrisy. For what had it been, really? If a castaway, adrift for months at sea, has a thousand times counted the number of wood fibers that make up his raft, in order to keep sane, should he boast about it when he reaches land? That he had the tenacity to survive? And what of it? Who cared? Why should it matter to anyone how I had filled my poor brain those ten years, and why was that more important than how I had filled my stomach?”\n— Stanisław Lem, in “Return from the stars”\n\nTopological data analysis is an exotic animal.\nHow can one mix a field from abstract math (topology) with real world data? In topology, finite topological spaces are trivial and often ignored, but every real-world data is finite. To connect these two areas of study, we need to have tools that transform finite metric spaces into objects that topology can handle and say something interest about."
  },
  {
    "objectID": "topology.html#what-is-topology",
    "href": "topology.html#what-is-topology",
    "title": "2  Topology",
    "section": "2.1 What is topology?",
    "text": "2.1 What is topology?\nTopology is the study of topological spaces and its properties. Unfortunately, this is not enough to close this chapter.\nA topological space can be thought of as a set with little “scales” called open sets. Each of these open sets represent the notion of local continuity or connectivity that we sense when we look at an object: the feeling that it is just “one single piece of a thing”, like the scales in a fish. We can stretch these scales, but can’t rip them apart. In a sense, topological spaces are elastic. Because of that, there is an old joke that a topologist can’t distinguish between a cup and a donut.\n\n\n\nFigure 2.1: A continuous transformation of a cup to a donut. A true topologist will try to bite all these cups. Source: Wikipedia.1\n\n\nMore formally,\n\nDefinition 2.1 A topological space is a pair \\((X, \\tau)\\) where\n\n\\(X\\) is a set;\n\\(\\tau\\) is a set of subsets of \\(X\\), that is: each \\(U \\in \\tau\\) is a set \\(U \\subseteq X\\).\n\nThe set \\(\\tau\\) has the following properties:\n\nthe intersection of a finite number of open sets is also an open set;\nthe union of an arbitrary (even infinity) number of open sets is also an open set;\nthe \\(\\emptyset\\) and \\(X\\) are open sets.\n\nThe set \\(\\tau\\) is called a topology on \\(X\\).\n\nWe often omit \\(\\tau\\) and simply write “a topological space \\(X\\)”. Sometimes we omit the “topological” too. We love omiting!\n\nDefinition 2.2 The set of all subsets (or “parts”) of \\(X\\) is denoted by \\(P(X)\\), that is,\n\\[\nP(X) = \\{ U \\; | \\; U \\subseteq X \\}.\n\\]\n\nGiven a set \\(X\\), there is a easy way to generate a topology on it:\n\nDefinition 2.3 Let \\(X\\) be a set and \\(S \\subseteq P(X)\\). Define \\(\\tau\\) \\(\\{\\emptyset, X\\}\\) united with finite intersections and arbitrary unions of elements of \\(S\\). The pair \\((X, \\tau)\\) is a topological space, and \\(\\tau\\) is said to be “generated by \\(S\\)”. We also say that \\(S\\) is a generator set for \\(\\tau\\).\n\nDon’t let the abstractions hurt you! Whenever you see a new definition or theorem, think of an example of objects that fit in it. One very useful example is the following:\n\nExample 2.1 The standard topology on the set \\(\\mathbb{R}\\) of real numbers is generated by the open intervals \\((a, b) \\subseteq \\mathbb{R}\\).\n\nIn the above case, an open set is a set in which one can always “walk a little more without reaching the end of the set”. More precisely, given \\(x \\in (a, b)\\) there is always a small enough \\(\\epsilon &gt; 0\\) such the interval \\((x - \\epsilon, x + \\epsilon)\\) is contained in \\((a, b)\\).\n!![fora de lugar] When an open set \\(U\\) cannot be written as the disjoint union of two open sets, we say that \\(U\\) is connected. These connected open sets give a notion of “being a single piece”."
  },
  {
    "objectID": "topology.html#continuity",
    "href": "topology.html#continuity",
    "title": "2  Topology",
    "section": "2.2 Continuity",
    "text": "2.2 Continuity\nWe learn in Calculus that a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is continuous when we can draw its graph “without lifting the pencil from the paper”. Formalizing this notion of continuity took centuries and made many mathematicians go crazy.2 Let’s try to formalize what is “to lift the pencil from the paper” with the following example:\n\n\n\nAn example of a discontinuous function. Source: byjus.com3\n\n\nWe notice that no matter how close to 3 we choose some \\(x \\in \\mathbb{R}\\), its image \\(f(x)\\) will end up being far away from \\(f(3)\\). A bit more precisely, there is a small interval around \\(f(3)\\), say \\[\n(f(3) - \\epsilon, f(3) + \\epsilon),\n\\]\nsuch that no matter how small we chose an interval around \\(3\\), say\n\\[\n(3 - \\delta, 3 + \\delta),\n\\]\nwe will have that\n\\[\nf(x) \\notin (f(3) - \\epsilon, f(3) + \\epsilon), \\; \\text{for some } x \\in (3 - \\delta, 3 + \\delta).\n\\]\nThis is equivalent to the fact that the inverse image of \\((f(3) - \\epsilon, f(3) + \\epsilon)\\) is not an open set containing 3: if it were an open set, we could always “walk a little” on this interval and still be sent by \\(f\\) to \\((f(3) - \\epsilon, f(3) + \\epsilon)\\).\nNow that we understood what is a discontinuous function, we define the following in the much more general context of topological spaces:\n\nDefinition 2.4 Given two topological spaces \\((X, \\tau)\\) and \\((Y, \\sigma)\\), a function \\(f: X \\to Y\\) is said to be continuous if the inverse image of any open set of \\(Y\\) is also an open set of \\(X\\). In symbols:\n\\[\n\\forall \\; V \\in \\sigma, \\; f^{-1}(V) \\in \\tau.\n\\]\n\n\nRemark. Negating what we obtained in our example above leads us to the definition of continuity of real functions, and to the most terrifying single line of math of first-year students: a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is continuous if for every point \\(a \\in \\mathbb{R}\\) we have\n\\[\n\\forall \\epsilon &gt; 0, \\exists \\delta &gt; 0 \\text{ such that } x \\in (a - \\delta, a + \\delta) \\text{ implies } f(x) \\in (f(a) - \\epsilon, f(a) + \\epsilon),\n\\]\nwhich is equivalent to \\(\\lim_{x \\to a} f(x) = a\\)."
  },
  {
    "objectID": "topology.html#the-notion-of-sameness-in-topology",
    "href": "topology.html#the-notion-of-sameness-in-topology",
    "title": "2  Topology",
    "section": "2.3 The notion of “sameness” in topology",
    "text": "2.3 The notion of “sameness” in topology\nLet’s digress a little.\nA topological space is completely define by a set \\(X\\) and its open sets \\(\\tau\\). If we just “rename” the elements of \\(X\\), then it is intuitive that this new topological space is as similar to \\(X\\) as possible; in fact, they are indistinguishable from each other: every topological property (ie: properties related to open sets) that the first has, the second also has.\nRenaming points is just a informally way to say “bijection of sets”. So, two topological spaces \\(X\\) and \\(Y\\) are “the same thing, topologically speaking” if there is a bijection \\(f: X \\to Y\\) such that \\(U \\subseteq X\\) is open if and only if \\(f(X) \\subseteq Y\\) is open. More precisely,\n\nDefinition 2.5 We say that two topological spaces \\(X\\) and \\(Y\\) are homeomorphic, and denote it by \\(X \\approx Y\\), if there exists a continuous bijection \\(f: X \\to Y\\) such that \\(f^{-1}\\) is also continuous. In this case, \\(f\\) is called an homeomorphism.\n\nTwo spaces are homeomorphic, then, if it is possible to deform one into another; this deformation include stretching and twisting, but never “gluing” points together (because of the bijection) and neither tearing them apart (because of the continuity). Note that the relation \\(\\approx\\) is\n\nsimmetric: \\(X \\approx X\\);\nreflexive: \\(X \\approx Y\\) implies \\(Y \\approx X\\);\ntransitive: \\(X \\approx Y\\) and \\(Y \\approx Z\\) implies \\(X \\approx Z\\).\n\n\nExample 2.2 The real line \\(\\mathbb{R}\\) is homeomorphic to a single interval \\((a, b)\\).\nProof: First, note that \\((a, b)\\) is homeomorphic to \\(I = (- \\pi / 2, \\pi / 2)\\). Consider then the map \\(\\tan: I \\to \\mathbb{R}\\), the tangent. Then \\(\\tan\\) is continuous, bijective and its inverse \\(\\tan^{-1}\\) is also continuous. Therefore, \\[\n(a, b) \\approx I \\approx \\mathbb{R}.\n\\]"
  },
  {
    "objectID": "topology.html#footnotes",
    "href": "topology.html#footnotes",
    "title": "2  Topology",
    "section": "",
    "text": "https://commons.wikimedia.org/wiki/File:Topology_joke.jpg↩︎\nI just invented this. Maybe some of them really got crazy, who knows?↩︎\nhttps://byjus.com/maths/discontinuity/↩︎\nie. \\(H(f)\\) will have the same notion of “sameness” in this context as homeomorphisms have in the context of topological spaces. So if \\(H(X)\\) is a vector space and \\(H(f)\\) is a linear transformation, then it is also an isomorphism of vector spaces; if \\(H(X)\\) is a group and \\(H(f)\\) is a homomorphism, then it is also an isomorphism of groups, and so on↩︎"
  },
  {
    "objectID": "simplicial.html",
    "href": "simplicial.html",
    "title": "3  Simplicial complexes",
    "section": "",
    "text": "“Are you a man, Octave? Do you see the leaves falling from the trees, the sun rising and setting? Do you hear the ticking of the horologe of time with each pulsation of your heart? Is there, then, such a difference between the love of a year and the love of an hour? I challenge you to answer that, you fool, as you sit there looking out at the infinite through a window not larger than your hand.”\n— Alfred de Musset, in “The confession of a child of the century”\n\nTopological spaces are nice, but all the interesting ones have an infinite amount of points: torus, circle, the real line, mobius band, projective plane, and so on. Topology usually is not interested in finite sets because their standard topology is trivial: just take every point as an open set.\nWe, as humans, can’t really grasp the infinite. Our universe is finite, and so ir our mind. To think about the infinite, we often use finite tricks. Take, for example, the way we prove something is valid for all the infinite natural numbers:\n\nfirst prove that a certain property \\(P\\) is true for 1;\nthen, prove that if it is valid for \\(n\\), then it is also valid for \\(n+1\\).\n\nPeano needed an axiom to guarantee that these 2 conditions are enough to prove that \\(P\\) is valid for all \\(\\mathbb{N}\\).\nfrom infinite to finite\nabstract simplicial complexes as approximation of other objects\nstandard embedding"
  },
  {
    "objectID": "homology.html",
    "href": "homology.html",
    "title": "4  Simplicial homology",
    "section": "",
    "text": "“Her thoughts were theorems, her words a problem,\nAs if she deem’d that mystery would ennoble ’em.”\n— Lord Byron, in “Don Juan”\n\nhomology as a tool to count holes; describe the algebra intuitively, formalise later"
  },
  {
    "objectID": "persistence.html",
    "href": "persistence.html",
    "title": "5  Persistent homology",
    "section": "",
    "text": "“The essence of this architecture is movement synchronized towards a precise objective. We observe a fraction of the process, like hearing the vibration of a single string in an orchestra of supergiants. We know, but cannot grasp, that above and below, beyond the limits of perception or imagination, thousands and millions of simultaneous transformations are at work, interlinked like a musical score by mathematical counterpoint. It has been described as a symphony in geometry, but we lack the ears to hear it.”\n— Stanisław Lem, in “Solaris”"
  },
  {
    "objectID": "digits.html#loading-packages",
    "href": "digits.html#loading-packages",
    "title": "11  Classifying hand-written digits",
    "section": "11.1 Loading packages",
    "text": "11.1 Loading packages\n\nimport MLDatasets\nusing Images, Makie, CairoMakie\nusing Distances\nusing Ripserer, PersistenceDiagrams\nusing StatsBase: mean\nimport Plots;\nusing DataFrames, FreqTables, PrettyTables\nusing Flux, ProgressMeter"
  },
  {
    "objectID": "digits.html#the-dataset",
    "href": "digits.html#the-dataset",
    "title": "11  Classifying hand-written digits",
    "section": "11.2 The dataset",
    "text": "11.2 The dataset\nMNIST is a dataset consisting of 70.000 hand-written digits. Each digit is a 28x28 grayscale image, that is: a 28x28 matrix of values from 0 to 1. To get this dataset, run\n\nn_train = 10_000\nmnist_digits, mnist_labels = MLDatasets.MNIST(split=:train)[:];\nmnist_digits = mnist_digits[:, :, 1:n_train]\nmnist_labels = mnist_labels[1:n_train];\n\nIf the console asks you to download some data, just press y.\nNotice that we only get the first n_train images so this notebook doesn’t take too much time to run. You can increase n_train to 60000 if you like to live dangerously and have enough RAM memory.\nNext, we transpose the digits and save them in a vector\n\nfigs = [mnist_digits[:, :, i]' |&gt; Matrix for i ∈ 1:size(mnist_digits)[3]];\n\nThe first digit, for example, is the following matrix:\n\nfigs[1]\n\n28×28 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.498039  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.25098   0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n ⋮                             ⋮         ⋱                 ⋮         \n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.215686  0.67451      0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.533333  0.992157     0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n\n\nWe can see a mosaic with the first 10^2 digits\n\nn = 10\nfigs_plot = [fig .|&gt; Gray for fig in figs[1:n^2]]\nmosaicview(figs_plot, nrow = n, rowmajor = true)"
  },
  {
    "objectID": "digits.html#preparing-for-war",
    "href": "digits.html#preparing-for-war",
    "title": "11  Classifying hand-written digits",
    "section": "11.3 Preparing for war",
    "text": "11.3 Preparing for war\nWhat topological tools can be useful to distinguish between different digits?\nPersistence homology with Vietoris-Rips filtration won’t be of much help: all digits are connected, so the 0-persistence is useless; for the 1-dimensional persistence,\n\n1, 3, 5, 7 do not contain holes;\n2 and 4 sometimes contain one hole (depending on the way you write it);\n0, 6, 9 contain one hole each;\n8 contains two holes.\n\nWhat if we starting chopping the digits with sublevels of some functions? The excentricity function is able to highlight edges. Doing a sublevel filtration with the excentricity function will permit us to separate digits by the amount of edges they have. So 1 and 3 and 7, for example, will have different persistence diagrams.\n\n11.3.1 From square matrices to points in the plane\nIn order to calculate the excentricity, we need to convert the “image digits” (28x28 matrices) to points in \\(\\mathbb{R}^2\\) (matrices with 2 columns, one for each dimension, which we will call pointclouds). A simple function can do that:\n\nfunction img_to_points(img, threshold = 0.3)\n    ids = findall(x -&gt; x &gt;= threshold, img)\n    pts = getindex.(ids, [1 2])\nend;\n\nNotice that we had to define a threshold: coordinates with values less than the threshold are not considered.\nLet’s also define a function to plot a digit:\n\nfunction plot_digit(fig, values = :black)\n    pt = img_to_points(fig)\n    f = Figure();\n    ax = Makie.Axis(f[1, 1], autolimitaspect = 1, yreversed = true)\n    scatter!(ax, pt[:, 2], pt[:, 1]; markersize = 40, marker = :rect, color = values)\n    if values isa Vector{&lt;:Real}\n        Colorbar(f[1, 2])\n    end\n    f\nend;\n\nWe can see that it works as expected\n\nfig = figs[3]\nheatmap(fig)\n\n\n\n\nbut the image is flipped. This is easily fixed:\n\nheatmap(fig |&gt; rotr90)\n\n\n\n\n\n\n11.3.2 Excentricity\nGetting into details: the excentricity of a metric space \\((X, d)\\) is a measure of how far a point is from the “center”. It is defined as follows for each \\(x \\in X\\):\n\\[\ne(x) = \\sum_{y \\in X} \\frac{d(x, y)}{N}\n\\]\nwhere \\(N\\) is the amount of points of \\(X\\).\nDefine a function that takes a digit in \\(\\mathbb{R}^2\\) and return the excentricity as an 28x28 image\n\nfunction excentricity(fig)\n    pt = img_to_points(fig)\n    dists = pairwise(Euclidean(), pt')\n    excentricity = [mean(c) for c ∈ eachcol(dists)]\n    exc_matrix = zeros(28, 28)\n\n    for (row, (i, j)) ∈ enumerate(eachrow(pt))\n        exc_matrix[i, j] = excentricity[row]\n    end\n\n    return exc_matrix\nend;\n\nWe store all the excentricities in the excs vector\n\nexcs = excentricity.(figs);\n\nand plot a digit with it’s corresponding excentricity\n\ni = 5\nfig = figs[i]\nexc = excs[i]\nheatmap(exc |&gt; rotr90)\n\n\n\n\nLooks good! Time to chop it.\n\n\n11.3.3 Persistence images\nNow we calculate all the persistence diagrams using sublevel filtration. This can take some seconds. Julia is incredibly fast, but does not perform miracles (yet!).\n\npds = map(excs) do ex\n    m = maximum(ex)\n    ex = m .- ex\n    ripserer(Cubical(ex), cutoff = 0.5)\nend;\n\nWe check the first one\n\npd = pds[i]\npd |&gt; barcode\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare it with the corresponding heatmap above. There are 3 main edges (and one really small one). It seems ok!\nWe can see the “step-by-step” creation of these connected components in the following mosaic.\n\nr = range(minimum(exc), maximum(exc), length = 25) |&gt; reverse\nfigs_filtration = map(r) do v\n    replace(x -&gt; x ≤ v ? 0 : 1, exc) .|&gt; Gray\nend\n\nmosaicview(figs_filtration..., rowmajor = true, nrow = 5, npad = 20)\n\n\n\n\nNow we create the persistence images of all these barcodes in dimension 0 and 1. We pass the entire collection of barcodes to the PersistenceImage function, and it will ensure that all of them are comparable (ie. are on the same grid).\n\npds_0 = pds .|&gt; first\npds_1 = pds .|&gt; last\nimgs_0 = PersistenceImage(pds_0; sigma = 1, size = 8)\nimgs_1 = PersistenceImage(pds_1; sigma = 1, size = 8);\n\nThe persistence images look ok too:\n\nPlots.plot(\n    barcode(pds[i])\n    , Plots.plot(pds[i]; persistence = true)\n    , Plots.heatmap(imgs_0(pds[i][1]); aspect_ratio=1)\n    ,  Plots.heatmap(imgs_1(pds[i][2]); aspect_ratio=1)\n    , layout = (2, 2)\n    )\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\nTop left: the barcode of a digit with respect to sublevels using the excentricity function. Top right: the corresponding persistence diagram. Bottom: 0 and 1 dimensional persistence images. They create a pixelated view of the persistence diagram, using a gaussian blur."
  },
  {
    "objectID": "digits.html#fitting-a-model",
    "href": "digits.html#fitting-a-model",
    "title": "11  Classifying hand-written digits",
    "section": "11.4 Fitting a model",
    "text": "11.4 Fitting a model\nIn order to use these persistence images in a machine learning model, we first need to vectorize them, ie, transform them into a vector. Machine learning models love vectors! The easist way is to just concatenate the persistence images as follows:\n\nfunction concatenate_pds(imgs_0, pds_0, imgs_1, pds_1)\n    persims = [\n        [vec(imgs_0(pds_0[i])); vec(imgs_1(pds_1[i])) ] for i in 1:length(pds)\n        ]\n\n    X = reduce(hcat, persims)'\n    X\nend\n\nX = concatenate_pds(imgs_0, pds_0, imgs_1, pds_1)\ny = mnist_labels .|&gt; string;\n\nWe can see that X is a matrix with 10000 rows (the amount of digits) and 128 columns (the persistence images concatenated).\nIt was also important to convert the mnist_labels to strings, because we want to classify the digits (and not do a regression on them).\nWe now have a vector for each image. What can we do? We need a model that takes a large vector of numbers and try to predict the digit. Neural networks are excellent in finding non-linear relations on vectors. Let’s try one!\nCreate the layers\n\nfunction nn_model(X)\n  model = Chain(\n      Dense(size(X)[2] =&gt; 64)\n      ,Dense(64 =&gt; 10)\n  )\nend\n\nmodel = nn_model(X)\n\n\nChain(\n  Dense(128 =&gt; 64),                     # 8_256 parameters\n  Dense(64 =&gt; 10),                      # 650 parameters\n)                   # Total: 4 arrays, 8_906 parameters, 35.039 KiB.\n\n\n\nthe loader\n\ntarget = Flux.onehotbatch(y, 0:9 .|&gt; string)\nloader = Flux.DataLoader((X' .|&gt; Float32, target), batchsize=32, shuffle=true);\n\nthe optimiser\n\noptim = Flux.setup(Flux.Adam(0.01), model);\n\nand train it\n\n@showprogress for epoch in 1:100\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend;\n\nThe predictions can be made with\n\npred_y = model(X' .|&gt; Float32)\npred_y = Flux.onecold(pred_y, 0:9 .|&gt; string);\n\nAnd the accuracy\n\naccuracy = sum(pred_y .== y) / length(y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the train set was $accuracy %!\")\n\nThe accuracy on the train set was 70.3 %!\n\n\nNot bad, taking into account that we only used the excentricity sublevel filtration.\nThe confusion matrix is the following:\n\ntbl = freqtable(y, pred_y)\n\n10×10 Named Matrix{Int64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │  917     0    19     6     2    18    19     2    12     6\n1           │    0  1086     5     2     4     8     0    22     0     0\n2           │   20    26   523    34   124    50    20    78    82    34\n3           │    7    57    43   622    21   122    13   127    16     4\n4           │    0     2    64     4   773    11     2   109     8     7\n5           │    8    64    46   181     9   384    17   131     5    18\n6           │   15     4    10     4     6     5   739    70    10   151\n7           │    2    51    29    17   140    25    56   748     2     0\n8           │   13     5    72     8    25     0     6     7   776    32\n9           │   12     1    21     9    16    10   330   104    13   462\n\n\nCalculating the proportion of prediction for each digit, we get\n\nround2(x) = round(100*x, digits = 1)\n\nfunction prop_table(y1, y2)\n    tbl = freqtable(y1, y2)\n    tbl_prop = prop(tbl, margins = 1) .|&gt; round2\n    tbl_prop\nend\n\ntbl_p = prop_table(y, pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 91.6   0.0   1.9   0.6   0.2   1.8   1.9   0.2   1.2   0.6\n1           │  0.0  96.4   0.4   0.2   0.4   0.7   0.0   2.0   0.0   0.0\n2           │  2.0   2.6  52.8   3.4  12.5   5.0   2.0   7.9   8.3   3.4\n3           │  0.7   5.5   4.2  60.3   2.0  11.8   1.3  12.3   1.6   0.4\n4           │  0.0   0.2   6.5   0.4  78.9   1.1   0.2  11.1   0.8   0.7\n5           │  0.9   7.4   5.3  21.0   1.0  44.5   2.0  15.2   0.6   2.1\n6           │  1.5   0.4   1.0   0.4   0.6   0.5  72.9   6.9   1.0  14.9\n7           │  0.2   4.8   2.7   1.6  13.1   2.3   5.2  69.9   0.2   0.0\n8           │  1.4   0.5   7.6   0.8   2.6   0.0   0.6   0.7  82.2   3.4\n9           │  1.2   0.1   2.1   0.9   1.6   1.0  33.7  10.6   1.3  47.2\n\n\nWe see that the biggest errors are the following:\n\nfunction top_errors(tbl_p)\n    df = DataFrame(\n        Digit = Integer[]\n        , Prediction = Integer[]\n        , Percentage = Float64[]\n        )\n    \n    for i = eachindex(IndexCartesian(), tbl_p)\n        push!(df, (i[1]-1, i[2]-1, tbl_p[i]))    \n    end   \n   \n    filter!(row -&gt; row.Digit != row.Prediction, df)\n    sort!(df, :Percentage, rev = true)\n    df[1:10, :]    \nend\n\ndf_errors = top_errors(tbl_p)\ndf_errors |&gt; pretty_table\n\n┌─────────┬────────────┬────────────┐\n│   Digit │ Prediction │ Percentage │\n│ Integer │    Integer │    Float64 │\n├─────────┼────────────┼────────────┤\n│       9 │          6 │       33.7 │\n│       5 │          3 │       21.0 │\n│       5 │          7 │       15.2 │\n│       6 │          9 │       14.9 │\n│       7 │          4 │       13.1 │\n│       2 │          4 │       12.5 │\n│       3 │          7 │       12.3 │\n│       3 │          5 │       11.8 │\n│       4 │          7 │       11.1 │\n│       9 │          7 │       10.6 │\n└─────────┴────────────┴────────────┘\n\n\n\n11.4.1 The perils of isometric spaces\nHow to separate “6” and “9”? They are isometric! For some people, “2” and “5” are also isometric (just mirror on the x-axis). Functions that only “see” the metric (like the excentricity) will never be able to separate these digits. In digits, the position of the features is important, so let’s add more slicing filtrations to our arsenal.\nTo avoid writing all the above code-blocks again, we encapsulate the whole process into a function\n\nfunction whole_process(\n    mnist_digits, mnist_labels, f\n    ; imgs_0 = nothing, imgs_1 = nothing\n    , dim_max = 1, sigma = 1, size_persistence_image = 8\n    )\n    figs = [mnist_digits[:, :, i]' |&gt; Matrix for i ∈ 1:size(mnist_digits)[3]]\n\n    excs = f.(figs);\n\n    pds = map(excs) do ex\n        m = maximum(ex)\n        ex = m .- ex\n        ripserer(Cubical(ex), cutoff = 0.5, dim_max = dim_max)\n    end;\n\n    pds_0 = pds .|&gt; first\n    pds_1 = pds .|&gt; last\n\n    if isnothing(imgs_0) \n        imgs_0 = PersistenceImage(pds_0; sigma = sigma, size = size_persistence_image) \n    end\n    if isnothing(imgs_1) \n        imgs_1 = PersistenceImage(pds_1; sigma = sigma, size = size_persistence_image) \n    end\n\n    persims = [\n    [vec(imgs_0(pds_0[i])); vec(imgs_1(pds_1[i])) ] for i in eachindex(pds)\n    ]\n\n    X = reduce(hcat, persims)'\n    y = mnist_labels .|&gt; string\n\n    return X, y, pds_0, pds_1, imgs_0, imgs_1\nend;\n\nWe now create the sideways filtrations: from the side and from above.\n\nset_value(x, threshold = 0.5, value = 0) = x ≥ threshold ? value : 0\n\nfunction filtration_sideways(fig; axis = 1, invert = false)\n\n  fig2 = copy(fig)\n  if axis == 2 fig2 = fig2' |&gt; Matrix end\n\n  for i ∈ 1:28\n    if invert k = 29 - i else k = i end\n    fig2[i, :] .= set_value.(fig2[i, :], 0.5, k)\n  end\n\n  fig2\n\nend;\n\nand calculate all 4 persistence diagrams. Warning: this can take a few seconds if you are using 60000 digits!\n\nfs = [\n    x -&gt; filtration_sideways(x, axis = 1, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = true)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = true)\n]\n\nret = @showprogress map(fs) do f\n    whole_process(\n        mnist_digits, mnist_labels, f\n        ,size_persistence_image = 8\n    )\nend;\n\nWe concatenate all the vectors\n\nX_list = ret .|&gt; first\nX_all = hcat(X, X_list...);\n\nand try again with a new model:\n\nmodel = nn_model(X_all)\n\ntarget = Flux.onehotbatch(y, 0:9 .|&gt; string)\nloader = Flux.DataLoader((X_all' .|&gt; Float32, target), batchsize=64, shuffle=true);\n\noptim = Flux.setup(Flux.Adam(0.01), model)\n\n@showprogress for epoch in 1:50\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend;\n\nNow we have\n\npred_y = model(X_all' .|&gt; Float32)\npred_y = Flux.onecold(pred_y, 0:9 .|&gt; string)\n\naccuracy = sum(pred_y .== y) / length(y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the train set was $accuracy %!\")\n\nThe accuracy on the train set was 95.1 %!\n\n\nwhich is certainly an improvement!\nThe proportional confusion matrix is\n\nprop_table(y, pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 99.3   0.0   0.1   0.1   0.0   0.0   0.3   0.0   0.2   0.0\n1           │  0.0  97.5   0.5   0.5   0.2   0.3   0.0   0.9   0.1   0.0\n2           │  0.3   0.5  85.4   2.9   0.1   6.9   1.6   1.9   0.4   0.0\n3           │  0.3   0.3   0.7  95.4   0.0   1.3   0.1   1.7   0.2   0.0\n4           │  0.1   0.0   0.0   0.0  98.9   0.0   0.4   0.3   0.1   0.2\n5           │  0.2   0.7   2.4   2.3   0.1  91.9   2.1   0.1   0.0   0.1\n6           │  0.0   0.2   0.1   0.0   0.1   0.0  99.6   0.0   0.0   0.0\n7           │  0.5   2.3   1.4   3.0   0.7   0.4   0.2  91.4   0.0   0.2\n8           │  0.1   0.1   0.5   0.0   1.3   0.1   0.8   0.0  96.8   0.2\n9           │  0.5   0.1   0.1   0.3   1.5   0.5   1.1   0.5   1.0  94.3"
  },
  {
    "objectID": "digits.html#learning-from-your-mistakes",
    "href": "digits.html#learning-from-your-mistakes",
    "title": "11  Classifying hand-written digits",
    "section": "11.5 Learning from your mistakes",
    "text": "11.5 Learning from your mistakes\nLet’s explore a bit where the model is making mistakes. Collect all the errors\n\nerrors = findall(pred_y .!= y);\n\nand plot the first 3\n\ni = errors[1]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 2 but it was a 1\n\n\n\n\n\n\ni = errors[2]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 5 but it was a 2\n\n\n\n\n\n\ni = errors[3]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 5 but it was a 2\n\n\n\n\n\nWe can make a mosaic with the first 100 errors\n\nn = 10\nfigs_plot = [figs[i] .|&gt; Gray for i in errors[1:n^2]]\nmosaicview(figs_plot, nrow = n, rowmajor = true)\n\n\n\n\nMany of these digits are really ugly! This makes them hard to classify with our sublevel filtrations. Some other functions could be explored."
  },
  {
    "objectID": "digits.html#getting-new-data",
    "href": "digits.html#getting-new-data",
    "title": "11  Classifying hand-written digits",
    "section": "11.6 Getting new data",
    "text": "11.6 Getting new data\nNow we want to see if our model really learned something, or if it just repeated what he saw in the training data. To check data, we need to get new data and calculate the accuracy of the same model on this new data.\n\nn_test = 5_000\nnew_mnist_digits, new_mnist_labels = MLDatasets.MNIST(split=:test)[:];\nnew_mnist_digits = new_mnist_digits[:, :, 1:n_test]\nnew_mnist_labels = new_mnist_labels[1:n_test];\n\nand obtaning X and y to feed the model\n\nfs = [\n    x -&gt; excentricity(x)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = true)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = true)\n]\n\nret = @showprogress map(fs) do f\n    whole_process(\n        new_mnist_digits, new_mnist_labels, f\n        ,size_persistence_image = 8\n    )\nend;\n\nDefine our new X and y\n\nnew_X = ret .|&gt; first\nnew_X = hcat(new_X...)\nnew_y = ret[1][2]\n\nnew_pred_y = model(new_X' .|&gt; Float32)\nnew_pred_y = Flux.onecold(new_pred_y, 0:9 .|&gt; string);\n\nand calculate the accuracy:\n\naccuracy = sum(new_pred_y .== new_y) / length(new_y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the test data was $accuracy %!\")\n\nThe accuracy on the test data was 87.1 %!\n\n\nA bit less than the training set, but not so bad.\nLet’s check the confusion matrix\n\ntbl = prop_table(new_y, new_pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 95.7   0.0   0.9   0.2   0.9   0.2   0.9   0.0   1.1   0.2\n1           │  0.5  95.4   1.1   0.4   0.9   0.5   0.2   1.1   0.0   0.0\n2           │  1.1   0.4  78.5   3.8   1.1   9.8   2.1   1.7   1.5   0.0\n3           │  0.0   1.0   5.8  85.0   0.0   3.8   0.2   3.4   0.8   0.0\n4           │  0.0   0.2   1.2   0.2  95.0   0.8   0.4   0.4   0.8   1.0\n5           │  0.4   0.9  14.9   4.2   2.2  73.5   0.9   1.1   1.5   0.4\n6           │  1.5   0.2   1.5   0.2   1.5   1.3  91.8   0.0   1.7   0.2\n7           │  0.2   3.1   3.7   4.7   6.6   1.4   0.2  79.3   0.0   0.8\n8           │  1.0   0.0   4.7   0.2   0.8   0.6   0.0   0.0  91.4   1.2\n9           │  0.2   0.2   1.0   0.2   5.8   0.4   0.6   2.1   4.6  85.0"
  },
  {
    "objectID": "digits.html#closing-remarks",
    "href": "digits.html#closing-remarks",
    "title": "11  Classifying hand-written digits",
    "section": "11.7 Closing remarks",
    "text": "11.7 Closing remarks\nEven though we used heavy machinery from topology, at the end our persistence images were vectors that indicated the birth and death of edges. Apart from that, the only machine learning algorithm we used was a simple dense neural network to fit these vectors to the correct labels in a non-linear way. State-of-art machine learning models on the MNIST dataset usually can get more than 99% of accuracy, but they use some complicated neural networks with many layers, and the output prediction are hard to explain. These methods, however, are not excludent of each other: we can use the persistence images (and any other vectorized output from TDA) together with other algorithms.\nA curious exercise to the reader is to check if a neural network with two parallel inputs - one for the digits images, followed by convolutional layers - other for the vector of persistence images, followed by dense layers can achieve a better result than the convolutional alone."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topological Data Analysis with Julia",
    "section": "",
    "text": "Preface\n\n“[O]f all the several ways of beginning a book which are now in practice throughout the known world, I am confident my own way of doing it is the best—I’m sure it is the most religious—for I begin with writing the first sentence—and trusting to Almighty God for the second.”\n— Laurence Sterne, in “The Life and Opinions of Tristram Shandy, Gentleman”\n\nWelcome!\nThis is the first draft of “Topological data analysis with Julia”.\nThe secret knowledge of Topological Data Analysis (TDA, for short) is spread in hundreds of papers and a few books. None, however, gives a consistent treatment of topology, data analysis and examples with code. Code is essential to transform theory into real data analysis.\nThis book tries to fill this gap. In it, we will outline the main methods used to analyse data with topology, and try to give some non-trivial examples. Besides, it is a healthy way I found to practice Julia and study TDA again.\nThe readers who are afraid of Mathematics are urged to at least understand the intuitive notions of the definitions and results presented here. This is why I will give many informal descriptions of the ideas and objects before formalising them. Keep in mind, however, that Mathematics is the language that best describes abstractions and the use of logic, and you only can learn a language by using it. For those who love math, I hope the “intuitive notions” won’t seem too boring.\nThis book will teach the basics of topology and data analysis needed to understand TDA; unfortunately, we will not teach you Julia directly. For that, there are many excellent resources. See, for example:\n\nThink Julia\nJulia for Optimization and Learning\nData Science in Julia for Hackers\n\nYou can, however, learn something from the code examples and modify them to your needs."
  },
  {
    "objectID": "persistence.html#clustering-revisited",
    "href": "persistence.html#clustering-revisited",
    "title": "10  Persistent homology",
    "section": "10.1 Clustering revisited",
    "text": "10.1 Clustering revisited\nStaticians are smart people.\nDendrograms as a 0-dimensional persistence\nVietoris-Rips e primos"
  },
  {
    "objectID": "clustering.html#a-taste-of-topology",
    "href": "clustering.html#a-taste-of-topology",
    "title": "5  Clustering",
    "section": "5.2 A taste of topology",
    "text": "5.2 A taste of topology\nGiven a pointcloud \\(X\\), how topology can help us in creating clusters?\nThe ToMATo clustering algorithm needs the following ingredients:\n\na density function;\na proximity graph;\na parameter \\(\\tau\\) which can be seen as “how high can a slope be so we can consider it noise”.\n\n\n\n\nThe algorithm as stated in the original paper. Source: etc.\n\n\nLet’s understand this algorithm with an example.\n\n\n\n\nUltsch, Alfred, and Jörn Lötsch. 2020. “The Fundamental Clustering and Projection Suite (FCPS): A Dataset Collection to Test the Performance of Clustering and Data Projection Algorithms.” Data 5 (1). https://doi.org/10.3390/data5010013."
  },
  {
    "objectID": "clustering.html#datasets",
    "href": "clustering.html#datasets",
    "title": "5  Clustering",
    "section": "5.1 Datasets",
    "text": "5.1 Datasets\nThe datasets are taken from (Ultsch and Lötsch 2020)\n\n\n\n\nUltsch, Alfred, and Jörn Lötsch. 2020. “The Fundamental Clustering and Projection Suite (FCPS): A Dataset Collection to Test the Performance of Clustering and Data Projection Algorithms.” Data 5 (1). https://doi.org/10.3390/data5010013."
  },
  {
    "objectID": "clustering.html#tomato",
    "href": "clustering.html#tomato",
    "title": "5  Clustering",
    "section": "5.1 ToMATo",
    "text": "5.1 ToMATo\nGiven a pointcloud \\(X\\), how topology can help us in creating clusters?\nLet’s understand the ToMATo algorithm with an example and pretend we are actually inventing it. How fun!\nLet’s load some packages first\n\nusing ToMATo\nimport GeometricDatasets as gd\nusing AlgebraOfGraphics, \n# CairoMakie, \nGLMakie;\n\nSuppose \\(X\\) is the following dataset:\n\nX = hcat(randn(2, 800), randn(2, 800) .* 0.8 .+ (4, -4), randn(2, 100) .* 0.3 .- (2, -2))\ndf = (x1 = X[1, :], x2 = X[2, :])\nplt = data(df) * mapping(:x1, :x2)\ndraw(plt)\n\n\n\n\nWe can see two big clusters and possibly a small one.\nLet’s make a note-so-absurd supposition that our data is generated by some normal distribuitions (in our case, it really is; in general this can be false). Then, intuitively, the mean of each distribution is the point with the highest density, ie, with more points close to it, because it is in the center of the distribution.\n\n5.1.1 Density\nWe can estimate the density of a dataset as follows\n\nds = gd.density_estimation(X, h = 0.5)\n\ndf = (x1 = X[1, :], x2 = X[2, :], ds = ds)\nplt = data(df) * mapping(:x1, :x2, color = :ds)\ndraw(plt)\n\n\n\n\nThe precise definition of the density of a point \\(x \\in X\\) is\n\\[\n\\text{dens}(x) = \\dfrac{1}{|X|} \\cdot \\sum_{y \\in X} \\text{exp} \\left(-\\dfrac{d(x, y)^2}{h^2} \\right)\n\\]\nwhere \\(\\text{exp}(x) = e^x\\). Informally, we want that points close (ie. small distance) to many other points have a high density; this is way we calculate the distance from \\(x\\) to \\(y\\) and put it inside an exponential with a minus sign. We then calculate the mean of all these values.\nNow we put this density estimation on another axis and plot it\n\naxis = (type = Axis3, width = 800, height = 450)\ndf = (x1 = X[1, :], x2 = X[2, :], ds = ds)\nplt = data(df) * mapping(:x1, :x2, :ds, color = :ds)\ndraw(plt; axis = axis)\n\n\n\n\nGood!\n\n\n5.1.2 Climbing mount topology\nThe idea now is that given a point \\(x\\):\n\nif \\(x\\) is the highest point in its corresponding mountain, then it is a new cluster;\notherwise, we will we seek for the highest neighbor of \\(x\\), say \\(x'\\) and say that the cluster of \\(x\\) is the same of \\(x'\\).\n\nTo do that, we need to define a notion of “neighborhood” in this dataset. The easiest way is to define a graph whose vertex set is \\(X\\) and edges connect neighbor points. Fortunately, the ToMATo package has a function that does exactly that. You are welcome!\n\ng = proximity_graph(X, 0.2, max_k_ball = 4, k_nn = 3, min_k_ball = 2)\n\n{1700, 4793} undirected simple Int64 graph\n\n\nThe graph \\(g\\) above is constructed as follows: given \\(x \\in X\\), we create a ball with radius \\(0.2\\) around \\(X\\) and do the following:\n\nAre there less than 2 points in the ball? If yes, then we connect \\(x\\) to its 2 closest points; if no, we connect \\(x\\) to its 6 (at maximum) closest points in the ball. In short: if the ball has not the amount of points we stipulated, then we use knn search.\n\nThese numbers obviously are arbitrary and can be changed at will.\nLet’s see the result of our algorithm:\n\nX2 = vcat(X, ds')\nclusters, _ = tomato(X, g, ds, 0)\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nLook how many clusters! This is obviously wrong.\n\n\n5.1.3 The Comedy of Errors\nThis tragedy happened because we did not take into account the “false peaks”: peaks that are just a little slump (!!!) and not a real peak. To merge these false-peaks into the big ones, we need to add the following step:\nLet \\(\\tau\\) be a number that denotes how small a slump must be to be merged (we will show how to calculate \\(\\tau\\) below). Given \\(x \\in X\\), let \\(N\\) be the set of its neighbors higher than \\(x\\). Denote by \\(x_max\\) the highest point in \\(N\\), and \\(c_max\\) its cluster. Now, for each \\(y \\in N\\), ask the following:\n\nIs the difference of heights of \\(x\\) and \\(y\\) less than \\(\\tau\\)? If yes, we merge the cluster of \\(y\\) with the cluster of \\(x_max\\). Otherwise, do nothing.\n\n\nτ = 0.02\nclusters, _ = tomato(X, g, ds, τ, max_cluster_height = τ)\n\nX2 = vcat(X, ds')\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nNow we got something!\nBut how did we calculate this magic \\(\\tau\\)? Should we just go on guessing real numbers? I hope not!\nWe usually run the ToMATo algorithm twice. The first time, we put \\(\\tau = \\inf\\) and see how the montains of \\(X\\) merged: we plot the time each one of these mountains survived.\n\n_, births_and_deaths = tomato(X, g, ds, Inf)\nplot_births_and_deaths(births_and_deaths)\n\n\n\n\nChoosing a bigger \\(\\tau\\) (say 0.04) will also merge the small clusters on the left\n\nτ = 0.04\nclusters, _ = tomato(X, g, ds, τ, max_cluster_height = τ)\n\nX2 = vcat(X, ds')\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nAt the end of the day you will still need to define how bigger a slump you can accept before merging it.\n\n\n5.1.4 The formal algorithm\nAfter all this talk, maybe the original algorithm can be better understood\n\nFor the curious reader, the math above is easily translated to Julia\n\nfunction tomato(\n    X::PointCloud, g::Graph, ds::Vector{&lt;:Real}, τ::Real = Inf;\n    max_cluster_height::Real = 0\n    )\n    sorted_ids = sortperm(ds, rev = true)\n    clusters = zeros(Int64, size(X)[2])\n    births_and_deaths = Dict{Int64, Vector{&lt;: Real}}()\n\n    for i ∈ sorted_ids\n        N = neighbors(g, i) |&gt; copy\n        filter!(x -&gt; ds[x] &gt; ds[i], N)\n    \n        # if there is no upper-neighbor\n        if length(N) == 0\n            clusters[i] = i\n            births_and_deaths[i] = [ds[i], Inf]\n            continue\n        end\n    \n        c_max = clusters[argmax(x -&gt; ds[x], N)]\n        clusters[i] = c_max\n        \n        for j ∈ N\n            c_j = clusters[j]\n    \n            # if the clusters are equal, skip\n            c_max == c_j && continue\n    \n            # if c_j has no cluster, put j on c_max\n            if c_j == 0\n                update_cluster!(clusters, j, c_max)\n                continue\n            end\n    \n            # If the lowest of them is just a bit below the current height ds[i],        \n            # we merge the clusters        \n            if min(ds[c_max], ds[c_j]) &lt; ds[i] + τ\n                from, to = sort([c_max, c_j], by = x -&gt; ds[x])\n                births_and_deaths[from][2] = ds[i]\n                replace!(clusters, from =&gt; to)\n            end\n        end    \n    end\n\n    sorted_clusters = sort(clusters |&gt; unique, by = x -&gt; ds[x], rev = true)\n\n    cluster_dict = \n        map(sorted_clusters) do cl\n\n            if  (ds[cl] &lt; max_cluster_height)\n                true_number = 0\n            else\n                true_number = findfirst(x -&gt; x == cl, sorted_clusters)\n            end\n\n            Dict(cl =&gt; true_number)\n        end\n    \n    cluster_dict = merge(cluster_dict...)\n\n    final_clusters = replace(clusters, cluster_dict...)\n\n    return final_clusters, births_and_deaths\n    \nend"
  },
  {
    "objectID": "mapper-general.html#what-is-a-mapper-algorithm",
    "href": "mapper-general.html#what-is-a-mapper-algorithm",
    "title": "9  Mapper: the general case",
    "section": "9.1 What is a mapper algorithm?",
    "text": "9.1 What is a mapper algorithm?\nWe can boil down the two mapper algorithms we saw earlier as follows:\n\n(covering step) Given a metric space \\((X, d)\\), create a covering \\(C\\) of \\(X\\);\n(nerve step) Using \\(C\\) as vertex set, create a graph.\n\nIn the classical mapper context, \\(C\\) is generated using the clustering of pre-images of a function \\(f: X \\to \\mathbb{R}\\). In the ball mapper scenario, we cover \\(X\\) using \\(\\epsilon\\)-balls with centers as a subset of \\(X\\).\n\n9.1.1 Covering step\nLet \\(X\\), \\(L\\) and \\(\\epsilon\\) be given as in the ball mapper case. For any \\(l \\in L\\), define \\(x_l\\) = X[:, l]. Examples of how to generalize the covering step:\n\nFix \\(n &gt; 0\\) integer. Create a ball \\(B\\) of radius \\(\\epsilon\\) around \\(x_l\\). If \\(B\\) contains less than \\(n\\) elements, then we redefine \\(B\\) as the set of \\(n\\) nearest neighbors of \\(x_l\\).\nFix \\(\\lambda &gt; 0\\). Let \\(d_l\\) be the distance between \\(x_l\\) and its closest point. Create a ball \\(B\\) of radius \\(\\lambda * d_1\\). Proceed like this to create a covering of \\(X\\).\n\n\n\n9.1.2 Nerve step\nThere are many alternatives to the nerve construction. Let \\(a\\) and \\(b\\) be two elements of a covering \\(C\\). Let \\(G = (V, E)\\) be a graph with vertex-set \\(V = C\\). Examples of how to generalize the nerve step:\n\nFix \\(k &gt; 0\\). Define \\((a, b) \\in E\\) iff \\(|a \\cap b| \\geq k\\), that is: we will only allow intersections with at least \\(k\\) elements. Setting \\(k = 1\\) will give us the usual nerve graph."
  },
  {
    "objectID": "mapper-general.html#creating-my-own-mapper",
    "href": "mapper-general.html#creating-my-own-mapper",
    "title": "9  Mapper: the general case",
    "section": "9.2 Creating my own mapper",
    "text": "9.2 Creating my own mapper\nYou can change the 2 steps above within the context of the ball mapper, as can be seen in the docs.\n\n\n\n\n\n\n\n\nball_mapper_generic(\n    X::PointCloud, L::Vector{&lt;:Integer}, \n    covering_function::Function,\n    graph_function::Function\n    )\nCreates the ball mapper of a metric space X subsampled by L.\n\n9.2.1 Arguments\n\nX::PointCloud: a point cloud.\nL::Vector{&lt;:Integer}: a subset of index of X, that is: L is a subset of [1:size(X)[2]].\ncovering_function::Function: a function that creates a cover for X. Its arguments are X and L.\ngraph_function::Function: a function that creates a graph. It accepts a CoveredPointCloud object.\n\n\n\n9.2.2 Details\nSee the “Generalization” page of the online documentation."
  },
  {
    "objectID": "tomato.html#inspiration",
    "href": "tomato.html#inspiration",
    "title": "6  ToMATo",
    "section": "6.1 Inspiration",
    "text": "6.1 Inspiration\nLet’s load some packages first\n\nusing ToMATo\nimport GeometricDatasets as gd\nusing AlgebraOfGraphics, \n# CairoMakie, \nGLMakie;\n\nSuppose \\(X\\) is the following dataset:\n\nX = hcat(randn(2, 800), randn(2, 800) .* 0.8 .+ (4, -4), randn(2, 100) .* 0.3 .- (2, -2))\ndf = (x1 = X[1, :], x2 = X[2, :])\nplt = data(df) * mapping(:x1, :x2)\ndraw(plt)\n\n\n\n\nWe can see two big clusters and possibly a small one.\nLet’s make a note-so-absurd supposition that our data is generated by some normal distribuitions (in our case, it really is; in general this can be false). Then, intuitively, the mean of each distribution is the point with the highest density, ie, with more points close to it, because it is in the center of the distribution.\n\n6.1.1 Density\nWe can estimate the density of a dataset as follows\n\nds = gd.density_estimation(X, h = 0.5)\n\ndf = (x1 = X[1, :], x2 = X[2, :], ds = ds)\nplt = data(df) * mapping(:x1, :x2, color = :ds)\ndraw(plt)\n\n\n\n\nThe precise definition of the density of a point \\(x \\in X\\) is\n\\[\n\\text{dens}(x) = \\dfrac{1}{|X|} \\cdot \\sum_{y \\in X} \\text{exp} \\left(-\\dfrac{d(x, y)^2}{h^2} \\right)\n\\]\nwhere \\(\\text{exp}(x) = e^x\\). Informally, we want that points close (ie. small distance) to many other points have a high density; this is way we calculate the distance from \\(x\\) to \\(y\\) and put it inside an exponential with a minus sign. We then calculate the mean of all these values.\nNow we put this density estimation on another axis and plot it\n\naxis = (type = Axis3, width = 800, height = 450)\ndf = (x1 = X[1, :], x2 = X[2, :], ds = ds)\nplt = data(df) * mapping(:x1, :x2, :ds, color = :ds)\ndraw(plt; axis = axis)\n\n\n\n\nGood!"
  },
  {
    "objectID": "tomato.html#climbing-mount-topology",
    "href": "tomato.html#climbing-mount-topology",
    "title": "6  ToMATo",
    "section": "6.2 Climbing mount topology",
    "text": "6.2 Climbing mount topology\nThe idea now is that given a point \\(x\\):\n\nif \\(x\\) is the highest point in its corresponding mountain, then it is a new cluster;\notherwise, we will we seek for the highest neighbor of \\(x\\), say \\(x'\\) and say that the cluster of \\(x\\) is the same of \\(x'\\).\n\nTo do that, we need to define a notion of “neighborhood” in this dataset. The easiest way is to define a graph whose vertex set is \\(X\\) and edges connect neighbor points. Fortunately, the ToMATo package has a function that does exactly that. You are welcome!\n\ng = proximity_graph(X, 0.2, max_k_ball = 6, k_nn = 3, min_k_ball = 3)\n\n{1700, 6154} undirected simple Int64 graph\n\n\nThe graph \\(g\\) above is constructed as follows: given \\(x \\in X\\), we create a ball with radius \\(0.2\\) around \\(X\\) and do the following:\n\nAre there less than 3 points in the ball? If yes, then we connect \\(x\\) to its 3 closest points; if no, we connect \\(x\\) to its 6 (at maximum) closest points in the ball. In short: if the ball has not the amount of points we stipulated, then we use knn search.\n\nThese numbers obviously are arbitrary and can be changed at will.\nLet’s see the result of our algorithm:\n\nX2 = vcat(X, ds')\nclusters, _ = tomato(X, g, ds, 0)\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nLook how many clusters! Something is rotten in the state of Denmark…"
  },
  {
    "objectID": "tomato.html#the-comedy-of-errors",
    "href": "tomato.html#the-comedy-of-errors",
    "title": "6  ToMATo",
    "section": "6.3 The Comedy of Errors",
    "text": "6.3 The Comedy of Errors\nThis tragedy happened because we did not take into account the “false peaks”: peaks that are just a little bump and not a real peak. To merge these small-peaks into the big ones, we need to add the following step:\nLet \\(\\tau\\) be a number that denotes how small a bump must be to be merged (we will show how to calculate \\(\\tau\\) below). Given \\(x \\in X\\), let \\(N\\) be the set of its neighbors higher than \\(x\\). Denote by \\(x_{max}\\) the highest point in \\(N\\), and \\(c_max\\) its cluster. Now, for each \\(y \\in N\\), ask the following:\n\nIs the difference of heights of \\(x\\) and \\(y\\) less than \\(\\tau\\)? If yes, we merge the cluster of \\(y\\) with the cluster of \\(x_{max}\\). Otherwise, do nothing.\n\n\nτ = 0.02\nclusters, _ = tomato(X, g, ds, τ, max_cluster_height = τ)\n\nX2 = vcat(X, ds')\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nNow we got something!\nBut how did we calculate this magic \\(\\tau\\)? Should we just go on guessing real numbers? I hope not!\nWe usually run the ToMATo algorithm twice. The first time, we put \\(\\tau = \\inf\\) and see how the montains of \\(X\\) merged: we plot the time each one of these mountains survived.\n\n_, births_and_deaths = tomato(X, g, ds, Inf)\nplot_births_and_deaths(births_and_deaths)\n\n\n\n\nChoosing a bigger \\(\\tau\\) (say 0.04) will also merge the small clusters on the left\n\nτ = 0.04\nclusters, _ = tomato(X, g, ds, τ, max_cluster_height = τ)\n\nX2 = vcat(X, ds')\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nAt the end of the day you will still need to define how bigger a bump you can accept before merging it.\n\n6.3.1 The formal algorithm\nAfter all this talk, maybe the original algorithm can be better understood!\n\n\n\nThe original ToMATo algorithm. Here, \\(G\\) is the proximity graph, \\(\\tilde{f}\\) is the density vector, \\(\\tau\\) is like above, \\(\\mathbfcal{N}\\) is the neighborhood of \\(x\\) with points higher than \\(x\\) (ie higher density), \\(r\\) stores the cluster of each point"
  },
  {
    "objectID": "mapper.html#some-theory",
    "href": "mapper.html#some-theory",
    "title": "7  (Classical) mapper",
    "section": "7.1 Some theory",
    "text": "7.1 Some theory\n\n7.1.1 Reeb graph\nIn topology, there are many ways by which we try to see what can’t be seen, in particular high-dimensional sets. The Reeb graph is one of those ways: given a topological space \\(X\\) and a continuous function \\(f: X \\to \\mathbb{R}\\), we can collapse the connected components of its pre-images to get a graph that reflects the level-sets of \\(f\\).\nMore formally, we define a relation \\(\\sim\\) on \\(X\\) such that \\(p \\sim q\\) if-and-only-if \\(p\\) and \\(q\\) belong to the same connected component of \\(f^{-1}(c)\\) for some \\(c \\in \\mathbb{R}\\).\n\n\n\nThe Reeb graph of a torus using the projection on the z-axis.\n\n\n\n\n7.1.2 The (classical) mapper\nThe (classical) mapper is an algorithm to create graphs from metric spaces, and can be seen as an “statistical” version of the Reeb graph.\nTo be able to mimick the Reeb graph, we need to change some objects from the continuous setting to the discrete setting:\n\n\\(X = (X, d)\\) is now a finite metric space, also called a point cloud;\n\\(f: X \\to \\mathbb{R}\\) can be any function (since \\(X\\) is discrete, \\(f\\) is automatically continuous);\ninstead of inverse images of points of \\(\\mathbb{R}\\), we calculate inverse images of subsets of \\(\\mathbb{R}\\) (usually intervals);\ninstead of connected components (which are trivial in the discrete setting), we use some clustering algorithm (DBSCAN, single linkage, etc.) and consider these clusterings as “connected pieces of \\(X\\)”.\n\n\nThe mapper graph can shed light to the geometry of \\(X\\):\n\nnodes are clusters of points of \\(X\\);\nthe color of the nodes can summarise some information about the points of \\(X\\) that represent this node;\nedges denote some proximity (in the metric of \\(d\\) of \\(X\\)) between the nodes.\n\nTo be more precise, to calculate the mapper of a metric space \\(X\\), we need the following ingredients:\n\na function \\(f: X \\to \\mathbb{R}\\) that measures something interesting in \\(X\\), as, for example, the excentricity, the first coordinate of PCA, and so on;\na covering \\(C\\) of the image \\(f(X) \\subset \\mathbb{R}\\);\na method \\(l\\) to cluster each \\(f^{-1}(c)\\) for \\(c \\in C\\).\n\nWhen all of this is chosen, we have a covering of \\(X\\) by clustering each pre-image of the elements of \\(C\\), that is:\n\\[\nV = \\{ l(p); \\; p = f^{-1}(c) \\; \\text{for} \\; c \\in C\\}\n\\]\nWe then calculate the 1-dimensional nerve of \\(V\\): we define the set of edges \\(E \\subset V \\times V\\) by\n\\[\n(v_1, v_2) \\in E \\leftrightarrow v_1 \\cap v_2 \\neq \\emptyset\n\\]\nIn words, we have an edge between \\(v_1\\) and \\(v_2\\) if there is some point in both \\(v_1\\) and \\(v_2\\) at the same time."
  },
  {
    "objectID": "mapper.html#less-theory-more-julia",
    "href": "mapper.html#less-theory-more-julia",
    "title": "7  (Classical) mapper",
    "section": "7.2 Less theory, more Julia!",
    "text": "7.2 Less theory, more Julia!\nLet’s import some packages:\n\nusing TDAmapper;\nimport GeometricDatasets as gd;\n\nand define \\(X\\) as a torus with the usual Euclidean distance\n\nX = gd.torus(2000)\n\n3×2000 Matrix{Float64}:\n -1.47279   -0.713709   2.37068   …  -0.20223    2.57742    2.30574\n  2.71141    3.80782   -3.09023      -3.03162   -2.5925    -0.666437\n -0.996331  -0.485686  -0.446418      0.999264  -0.755023   0.800092\n\n\nImportant: when using TDAmapper, your point cloud must be in column-major order. That is: each point of \\(X\\) must be a column of X, not a row (as is usual with dataframes). This is so because Distances.jl, NearestNeighbors.jl, Clustering.jl and many other packages for calculations with metric spaces use the column-major order for performance reasons.\nWe define the function \\(f: X \\to \\mathbb{R}\\) as the projection on the \\(x\\)-axis because our torus is laying down compared to the one in the Reeb graph example.\nLet fv be a vector such that fv[i] is the \\(x\\)-axis projection of the point \\(x_i\\) of \\(X\\):\n\nfv = X[1, :];\n\nYou can plot \\(X\\) colored by \\(f\\) as follows:\n\nusing CairoMakie;\nscatter(X[1, :], X[2, :], X[3, :], color = fv)\n\n\n\n\nImportant: the plots will be interactive when running in Julia if you change CairoMakie to GLMakie. Give it a try!\nDefine the covering intervals cv as follows:\n\nC = uniform(fv, overlap = 150);\n\nYou can check the first five intervals of this covering:\n\nC[1:5]\n\n5-element Vector{Interval}:\n Interval(-4.706857f0, -3.708342f0)\n Interval(-4.136277f0, -3.1377618f0)\n Interval(-3.565697f0, -2.5671818f0)\n Interval(-2.995117f0, -1.9966017f0)\n Interval(-2.4245367f0, -1.4260216f0)\n\n\nFor the clustering algorithm we choose the DBSCAN with radius 1:\n\nclustering = cluster_dbscan(radius = 1);\n\nThen the mapper graph of \\(X\\) can be calculated by\n\n# the mapper function needs:\n# X\n# the values of f(X)\n# the covering C\n# the clustering function\nmp = mapper(X, fv, C; clustering = clustering)\n\nAnd plotted with\n\n# define the value of each node as the maximum of\n# values of fv \nnode_values = node_colors(mp, fv)\n\nmapper_plot(mp, node_values = node_values)\n\n\n\n\nCompare it with the Reeb graph from the start. If this isn’t nice, what is?"
  },
  {
    "objectID": "topology.html#the-opposite-of-sameness",
    "href": "topology.html#the-opposite-of-sameness",
    "title": "2  Topology",
    "section": "2.4 The opposite of sameness",
    "text": "2.4 The opposite of sameness\nAll the topological spaces of figure Figure 2.1 are homeomorphic to each other. To show that, it is enough to exhibit a homeomorphism between them. But how can we prove that two spaces are not homeomorphic? How can we show that it is impossible to stretch and twist and rotate one object into another? There are can be infinite maps from one space to another!\nProving that spaces are homeomorphic (or not) is one of the central problems in topology, and many complex tools where created to solve it. Let’s see below one general idea to solve it.\nLet \\(X\\) and \\(Y\\) be two spaces. We suspect that they are not homeomorphic; to confirm our suspicion, suppose they are homeomorphic and let’s arrive at an absurd. So let \\(f: X \\to Y\\) be an homeomorphism. If we have some kind of “simplification map” \\(H\\) that transform \\(X\\) into a much simpler object \\(H(X)\\) (and equivalently \\(Y\\) into \\(H(Y)\\)), and, moreover, transform the map\n\\[\nf: X \\to Y\n\\]\ninto a map between these new objects\n\\[\nH(f): H(X) \\to H(Y)\n\\]\nthen we can start looking for some paradox in these new and much simpler objects.\nLet’s be even more optimistic: suppose that \\(H\\) is so well-behaved that \\(f\\) is transformed into an isomorphism4. It is enough, then, to prove that \\(H(X)\\) is not isomorphic to \\(H(Y)\\). We could simplify even more and demand that \\(H(X)\\) is just a number, and \\(H(f)\\) is an equality; then\n\\[\nH(X) \\text{\\; different from\\; } H(Y) \\Rightarrow \\text{$X$ and $Y$ are not homeomorphic}.\n\\]\nThis transformation of “topological spaces + maps” to “things + maps between things” is called a functor. We are starting to dip our toes into the mathematical field called category theory, which is a kind of meta-mathematics. We will see more about functors in the chapter about homology."
  },
  {
    "objectID": "topology.html#falar-de-teoria-de-categoria",
    "href": "topology.html#falar-de-teoria-de-categoria",
    "title": "2  Topology",
    "section": "2.5 Falar de teoria de categoria?",
    "text": "2.5 Falar de teoria de categoria?\n??????????????????????????"
  },
  {
    "objectID": "topology.html#category-theory",
    "href": "topology.html#category-theory",
    "title": "2  Topology",
    "section": "2.5 Category theory??????????",
    "text": "2.5 Category theory??????????\n??????????????????????????"
  },
  {
    "objectID": "mapper.html#reeb-graph",
    "href": "mapper.html#reeb-graph",
    "title": "7  (Classical) mapper",
    "section": "7.1 Reeb graph",
    "text": "7.1 Reeb graph\nIn topology, there are many ways by which we try to see what can’t be seen, in particular high-dimensional sets. The Reeb graph is one of those ways: given a topological space \\(X\\) and a map \\(f: X \\to \\mathbb{R}\\), we can collapse the connected components of its pre-images to get a graph that reflects the level-sets of \\(f\\).\nMore formally, we define a relation \\(\\sim\\) on \\(X\\) such that \\(p \\sim q\\) if-and-only-if \\(p\\) and \\(q\\) belong to the same connected component of \\(f^{-1}(c)\\) for some \\(c \\in \\mathbb{R}\\).\n\n\n\nThe Reeb graph of a torus using the projection on the z-axis. Source: Wikipedia1\n\n\n\n7.1.1 The (classical) mapper\nThe (classical) mapper is an algorithm to create graphs from metric spaces, and can be seen as an “statistical” version of the Reeb graph.\nTo be able to mimick the Reeb graph, we need to change some objects from the continuous setting to the discrete setting:\n\n\\(X = (X, d)\\) is now a finite metric space, also called a point cloud;\n\\(f: X \\to \\mathbb{R}\\) can be any function (since \\(X\\) is discrete, \\(f\\) is always continuous);\ninstead of inverse images of points of \\(\\mathbb{R}\\), we calculate inverse images of subsets of \\(\\mathbb{R}\\) (usually intervals);\ninstead of connected components (which are trivial in the discrete setting), we use some clustering algorithm (DBSCAN, single linkage, etc.) and consider these clusterings as “connected pieces of \\(X\\)”.\n\n\nThe mapper graph can shed light to the geometry of \\(X\\):\n\nnodes are clusters of points of \\(X\\);\nthe color of the nodes can summarise some information about the points of \\(X\\) that represent this node;\nedges denote some proximity (in the metric of \\(d\\) of \\(X\\)) between the nodes.\n\nTo be more precise, to calculate the mapper of a metric space \\(X\\), we need the following ingredients:\n\na function \\(f: X \\to \\mathbb{R}\\) that measures something interesting in \\(X\\), as, for example, the excentricity, the first coordinate of PCA, and so on;\na covering \\(C\\) of the image \\(f(X) \\subset \\mathbb{R}\\);\na method \\(l\\) to cluster each \\(f^{-1}(c)\\) for \\(c \\in C\\).\n\nWhen all of this is chosen, we have a covering of \\(X\\) by clustering each pre-image of the elements of \\(C\\), that is:\n\\[\nV = \\{ l(p); \\; p = f^{-1}(c) \\; \\text{for} \\; c \\in C\\}\n\\]\nWe then calculate the 1-dimensional nerve of \\(V\\): we define the set of edges \\(E \\subset V \\times V\\) by\n\\[\n(v_1, v_2) \\in E \\leftrightarrow v_1 \\cap v_2 \\neq \\emptyset\n\\]\nIn words, we have an edge between \\(v_1\\) and \\(v_2\\) if there is some point in both \\(v_1\\) and \\(v_2\\) at the same time."
  },
  {
    "objectID": "mapper.html#an-example-in-julia",
    "href": "mapper.html#an-example-in-julia",
    "title": "7  (Classical) mapper",
    "section": "7.2 An example in Julia",
    "text": "7.2 An example in Julia\nLet’s import some packages:\n\nusing TDAmapper;\nimport GeometricDatasets as gd;\n\nand define \\(X\\) as a torus with the usual Euclidean distance\n\nX = gd.torus(2000)\n\n3×2000 Matrix{Float64}:\n 3.31687   3.01965   2.86135   -1.18996   …  2.52688    3.98365     0.271879\n 1.99244   2.03354  -1.65949   -2.14445      0.570201  -0.360719    2.33906\n 0.494297  0.76792   0.951466   0.836795     0.912271   0.00970037  0.764024\n\n\nImportant: when using TDAmapper, your point cloud must be in column-major order. That is: each point of \\(X\\) must be a column of X, not a row (as is usual with dataframes). This is so because Distances.jl, NearestNeighbors.jl, Clustering.jl and many other packages for calculations with metric spaces use the column-major order for performance reasons.\nWe define the function \\(f: X \\to \\mathbb{R}\\) as the projection on the \\(x\\)-axis because our torus is laying down compared to the one in the Reeb graph example.\nLet fv be a vector such that fv[i] is the \\(x\\)-axis projection of the point \\(x_i\\) of \\(X\\):\n\nfv = X[1, :];\n\nYou can plot \\(X\\) colored by \\(f\\) as follows:\n\nusing CairoMakie;\nscatter(X[1, :], X[2, :], X[3, :], color = fv)\n\n\n\n\nImportant: the plots will be interactive when running in Julia if you change CairoMakie to GLMakie. Give it a try!\nDefine the covering intervals cv as follows:\n\nC = uniform(fv, overlap = 150);\n\nYou can check the first five intervals of this covering:\n\nC[1:5]\n\n5-element Vector{Interval}:\n Interval(-4.7024693f0, -3.7038724f0)\n Interval(-4.1318426f0, -3.1332457f0)\n Interval(-3.5612159f0, -2.562619f0)\n Interval(-2.9905891f0, -1.9919922f0)\n Interval(-2.4199622f0, -1.4213654f0)\n\n\nFor the clustering algorithm we choose the DBSCAN with radius 1:\n\nclustering = cluster_dbscan(radius = 1);\n\nThen the mapper graph of \\(X\\) can be calculated by\n\n# the mapper function needs:\n# X\n# fv, the values of f(X)\n# the covering C\n# the clustering function\nmp = mapper(X, fv, C; clustering = clustering)\n\nAnd plotted with\n\n# define the value of each node as the maximum of\n# values of fv \nnode_values = node_colors(mp, fv)\n\nmapper_plot(mp, node_values = node_values)\n\n\n\n\nCompare it with the Reeb graph from the start. If this isn’t nice, what is?"
  },
  {
    "objectID": "mapper.html#footnotes",
    "href": "mapper.html#footnotes",
    "title": "7  (Classical) mapper",
    "section": "",
    "text": "https://commons.wikimedia.org/wiki/File:3D-Leveltorus-Reebgraph.png↩︎"
  },
  {
    "objectID": "ballmapper.html#the-vietoris-rips-complex",
    "href": "ballmapper.html#the-vietoris-rips-complex",
    "title": "8  Ball mapper",
    "section": "8.1 The Vietoris-Rips complex",
    "text": "8.1 The Vietoris-Rips complex\nAnother way to reduce the complexity of a metric space is to approximate it by a simplicial complex. Simplicial complexes are like small building blocks glued together, each of these blocks a small representative of an \\(n\\)-dimensional space: points, line segments, triangles, tetrahedrons, and so on.\nThe Vietoris-Rips complex is build as follows: given a metric space \\((X, d)\\) and an \\(\\epsilon &gt; 0\\), define the following simplicial complex:\n\\[\nVR(X, \\epsilon) = \\{ [ x_1, \\ldots, x_n ] \\; d(x_i, x_j) &lt; \\epsilon, \\forall i, j \\}\n\\]\nthat is: the points of \\(X\\) are our vertices, and we have an \\(n\\)-simplex \\([x_1, \\ldots, x_n]\\) whenever the pairwise distance between \\(x_1, \\ldots, x_n\\) is less than \\(\\epsilon\\). This condition is equivalent to ask that\n\\[\n\\cap_i B(x_i, \\epsilon) \\neq \\emptyset\n\\]\nwhere \\(B(x, \\epsilon)\\) is the ball of center \\(x\\) and radius \\(\\epsilon\\).\n\n\n\nThe black dots are points in a metric space; the pink circles are \\(\\epsilon\\) balls around the points; in green, we have the Vietoris-Rips complex. Source: https://www.researchgate.net/publication/331739415_Topological_data_analysis_for_the_string_landscape"
  },
  {
    "objectID": "ballmapper.html#the-ball-mapper",
    "href": "ballmapper.html#the-ball-mapper",
    "title": "8  Ball mapper",
    "section": "8.2 The ball mapper",
    "text": "8.2 The ball mapper\nThe ball mapper is clearly inspired by the Vietoris-Rips complex. Given a metric space \\((X, d)\\) with \\(X = \\{x_1, \\ldots, x_n\\}\\), select a subset of indexes \\(L \\subseteq \\{1, \\ldots, n\\}\\) and define the ball mapper graph G as follows: the set of vertices of \\(G\\) is \\(L\\), and set of edges \\(E\\) given by\n\\[\n(i, j) \\in E \\Leftrightarrow B(x_i, \\epsilon) \\cap B(x_j, \\epsilon) \\neq \\emptyset\n\\]\nThe ball mapper then can be seen as the 1-skeleton of the Vietoris-Rips, but create using balls whose center can only be the elements indexed by \\(L\\).\nTo exemplify, consider a circle\n\nusing TDAmapper\nimport GeometricDatasets as gd\n\nX = gd.sphere(1000, dim = 2);\n\nCheck that it is indeed a circle:\n\nusing CairoMakie\nscatter(X)\n\n\n\n\nNow take \\(L\\) as a hundred random points and let’s create the ball mapper of \\(X\\) with radius \\(\\epsilon = 0.1\\):\n\nL = rand(1:1000, 100)\nmp = ball_mapper(X, L, ϵ = 0.5);\n\n\nmapper_plot(mp)\n\n\n\n\nThat’s quite a circle!"
  },
  {
    "objectID": "simplicial.html#footnotes",
    "href": "simplicial.html#footnotes",
    "title": "3  Simplicial complexes",
    "section": "",
    "text": "https://commons.wikimedia.org/wiki/File:Dominoeffect.png↩︎\nThe original text is in Latin, so I just invented this.↩︎\nhttps://en.wikipedia.org/wiki/File:Konigsberg_bridges.png↩︎\nhttps://en.wikipedia.org/wiki/File:7_bridges.svg↩︎\nhttps://en.wikipedia.org/wiki/File:K%C3%B6nigsberg_graph.svg↩︎\nSee https://en.wikipedia.org/wiki/Monster_group for more terror tales in mathematics.↩︎"
  },
  {
    "objectID": "simplicial.html#the-infinite-through-a-window",
    "href": "simplicial.html#the-infinite-through-a-window",
    "title": "3  Simplicial complexes",
    "section": "3.1 The infinite through a window",
    "text": "3.1 The infinite through a window\nTopological spaces are nice, but all the interesting ones have an infinite amount of points: torus, circle, the real line, mobius band, projective plane, and so on. Topology usually is not interested in finite sets because their standard topology is trivial: just take every point as an open set.\nWe, as humans, can’t really grasp the infinite. Our universe is finite, and so ir our mind. To think about the infinite, we need to use finite “tricks”. Take, for example, the way we prove something is valid for all the infinite natural numbers, a principle called finite induction:\n\nfirst prove that a certain property \\(P\\) is true for 1;\nthen, prove that if it is valid for \\(n\\), then it is also valid for \\(n+1\\).\n\nPeano needed an axiom to guarantee that these 2 conditions are enough to prove that \\(P\\) is valid for all \\(\\mathbb{N}\\).\n\n\n\nThe finite induction principle can be tought of as a domino falling and pushing the next piece. But with infinite domino pieces and infinite patience. Source: Wikipedia1\n\n\nAs another example, when studying linear algebra we see the concept of basis of a vector space \\(V\\). With basis, we can describe exactly any point \\(v \\in V\\) using a finite combination of its base elements, say \\(v = \\lambda_1 e_1 + \\ldots \\lambda_n e_n\\). The infinite amount of points in \\(V\\) can then be written as sums of finite objects that we can map mentally.\nFinding a finite representation of a mathematical object is often desired, and with topological spaces it was not different."
  },
  {
    "objectID": "simplicial.html#graphs",
    "href": "simplicial.html#graphs",
    "title": "3  Simplicial complexes",
    "section": "3.2 Graphs",
    "text": "3.2 Graphs\nGraphs were created by Euler in 1736 because he wanted to visit Königsberg but was too lazy2 to walk around like a normal person, and then tried to cross all its bridges just one time.\n\n\n\nCan you walk through the city crossing each of those bridges once and only once? Spoiler: no, you can’t. Stop trying! Source: Wikipedia3\n\n\nWe can abstract away this map with its bridges and just think about the bridges and the portions of land:\n\n\n\nThe map of Königsberg if the city was a green carpet. Source: Wikipedia4\n\n\nEuler did even better! He needed just two things to represent this object:\n\npoints: the portions of the cities;\nedges: bridges that connect two points.\n\nThe result is the following:\n\n\n\nA graph representing the original problem. Source: Wikipedia5\n\n\nOops, Euler just invented graphs!\nFormally,\n\nDefinition 3.1 A directed graph is a pair \\(G = (V, E)\\) where \\(V\\) is a set called vertices and \\(E \\subseteq V \\times V\\) is a set of edges between the vertices. An element \\((v, w) \\in E\\) can also be represented as \\(v \\to w\\).\n!!! falar sobre grafo indireto; usar {v, w} em vez de pares\n\nHe noticed that when you travel to a green point \\(v\\), you need to take another bridge to get out of \\(v\\). Thus, the number of edges need to be even for all the points we visit during the middle of our journey (excluding the beginning and the end). But all points in the above graph have an odd number of edges! Therefore, it is impossible to travel cross each bridge just once and still visit all the green points.\nGraphs can be used whenever we need to represent a set of objects and a pairwise relation between these objects.\n\n3.2.1 The essence of a circle\nWhat is a circle, really? The boring answer is “the set of points that dist \\(r\\) of a point \\(p\\)”. But in a topological view, a circle is just a 1-dimensional closed real interval with its extremities glued together, forming a hole inside.\nThe following graph, when seen as a subset of \\(\\mathbb{R}^2\\) is homeomorphic to a circle:\n\n\n\nFigure 3.1: A poorly drawn graph representing a circle.\n\n\nWith this horrendous graph6 we can represent a circle in a finite way: three points \\(a, b, c\\) and all possible edges: \\(a \\to b\\), \\(b \\to c\\), \\(c \\to a\\)."
  },
  {
    "objectID": "simplicial.html#simplicial-complexes",
    "href": "simplicial.html#simplicial-complexes",
    "title": "3  Simplicial complexes",
    "section": "3.3 Simplicial complexes",
    "text": "3.3 Simplicial complexes\nWhy stop with vertices and edges? Edges are just pairs of edges. Why not take triples and quadruples and so on?\nWell, now you’ve reinvented simplicial complexes! Congratulations!\n\nDefinition 3.2 A simplicial complex \\(\\Sigma\\) is a set of subsets of \\(X\\) with the following property:\n\nfor any \\(\\sigma \\in \\Sigma\\), every subset of \\(\\sigma\\) (also called a face of \\(\\sigma\\)) is also in \\(\\Sigma\\);\ngiven non-empty \\(\\sigma_1, \\sigma_2 \\in \\Sigma\\), the intersection \\(\\sigma_1 \\cap \\sigma_2\\) is also in \\(\\Sigma\\).\n\n\n!!!abstract simplicial complexes as approximation of other objects\n!!!standard embedding"
  }
]