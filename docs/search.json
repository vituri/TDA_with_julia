[
  {
    "objectID": "topology.html#what-is-topology",
    "href": "topology.html#what-is-topology",
    "title": "2  Topology",
    "section": "2.1 What is topology?",
    "text": "2.1 What is topology?\nTopology is the study of topological spaces. Unfortunately, this is not enough to close this chapter.\nA topological space can be thought of as a set with little “scales” called open sets. Each of these open sets represent the notion of continuity or connectivity that we sense when we look at an object: the feeling that it is just “one single piece of a thing”, like the scales in a fish.\n!!!melhorar aqui. inserir imagem sobre princípio de gestalt do livro de storytelling\n!!!inserir outras imagens?\nMore formally,\n\nDefinition 2.1 A topological space is a pair \\((X, \\tau)\\) where\n\n\\(X\\) is a set;\n\\(\\tau\\) is a set of subsets of \\(X\\), that is: each \\(U \\in \\tau\\) is a set \\(U \\subseteq X\\).\n\nThe set \\(\\tau\\) has the following properties:\n\nthe intersection of a finite number of open sets is also an open set;\nthe union of an arbitrary (even infinity) number of open sets is also an open set;\nthe \\(\\emptyset\\) and \\(X\\) are open sets.\n\n\nAn example that should be always brought to the mind is the set \\(\\mathbb{R}\\) of real numbers, with open sets given by the open intervals \\((a, b) \\subseteq \\mathbb{R}\\) (and its unions).\n!!! falar sobre topologia gerada por um subconjunto de \\(P(X)\\) tomando interseções e uniões.\nWhen an open set \\(U\\) cannot be written as the disjoint union of two open sets, we say that \\(U\\) is connected. These connected open sets give a notion of “being a single piece”."
  },
  {
    "objectID": "topology.html#introdução",
    "href": "topology.html#introdução",
    "title": "2  Topology",
    "section": "2.3 Introdução",
    "text": "2.3 Introdução\nTopologia: estudo dos objetos geométricos elásticos Dar exemplo da rosquinha e da caneca Comentar que é estranho que sejam “o mesmo objeto” Topologia enxerga o que? Dar exemplo do relógio: dois relógios bem diferentes podem dar a mesma hora Ideia de isomorfismo em categorias"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "“Ven you’re a married man, Samivel, you’ll understand a good many things as you don’t understand now; but vether it’s worth while goin’ through so much, to learn so little, as the charity-boy said ven he got to the end of the alphabet, is a matter o’ taste. I rayther think it isn’t.”\n— Charles Dickens, in “The Pickwick Papers”\n\nA = 2 kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk que issooooooooooooooooooo que textãooooooooooooooooo"
  },
  {
    "objectID": "simplicial.html",
    "href": "simplicial.html",
    "title": "3  Simplicial complexes",
    "section": "",
    "text": "“Thank God for giving you a glimpse of heaven, but do not imagine yourself a bird because you can flap your wings.”\n— Alfred de Musset, in “The confession of a child of the century”\n\nFalar sobre complexos simpliciais e sua aproximação da realidade"
  },
  {
    "objectID": "homology.html",
    "href": "homology.html",
    "title": "4  Simplicial homology",
    "section": "",
    "text": "Introduzir homologia como modo de contar buracos. Ideia intuitiva, depois formalização"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topological Data Analysis with Julia",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "digits.html#loading-packages",
    "href": "digits.html#loading-packages",
    "title": "6  Classifying hand-written digits",
    "section": "6.1 Loading packages",
    "text": "6.1 Loading packages\n\nimport MLDatasets\nusing Images, Makie, CairoMakie\nusing Distances\nusing Ripserer, PersistenceDiagrams\nusing StatsBase: mean\nimport Plots;\nusing FreqTables"
  },
  {
    "objectID": "digits.html#the-dataset",
    "href": "digits.html#the-dataset",
    "title": "6  Classifying hand-written digits",
    "section": "6.2 The dataset",
    "text": "6.2 The dataset\nMNIST is a dataset consisting of 70.000 hand-written digits. Each digit is a 28x28 grayscale image, that is: a 28x28 matrix of values from 0 to 1. To get this dataset, run\n\nn_train = 8_000\nmnist_digits, mnist_labels = MLDatasets.MNIST(split=:train)[:];\nmnist_digits = mnist_digits[:, :, 1:n_train]\nmnist_labels = mnist_labels[1:n_train];\n\nIf the console asks you to download some data, just press y.\nNotice that we only get the first n_train images so this notebook doesn’t take too much time to run. You can increase n_train to 60000 if you like to live dangerously and have enough RAM memory.\nNext, we transpose the digits and save them in a vector\n\n# store all digits in a figs variable\nfigs = [mnist_digits[:, :, i]' |&gt; Matrix for i ∈ 1:size(mnist_digits)[3]];\n\nThe first digit, for example, is the following matrix:\n\nfigs[1]\n\n28×28 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.498039  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.25098   0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n ⋮                             ⋮         ⋱                 ⋮         \n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.215686  0.67451      0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.533333  0.992157     0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n\n\nWe can see a mosaic with the first 10^2 digits\n\nn = 10\nfigs_plot = [fig .|&gt; Gray for fig in figs[1:n^2]]\nmosaicview(figs_plot, nrow = n, rowmajor = true)"
  },
  {
    "objectID": "digits.html#preparing-for-war",
    "href": "digits.html#preparing-for-war",
    "title": "6  Classifying hand-written digits",
    "section": "6.3 Preparing for war",
    "text": "6.3 Preparing for war\nWhat topological tools can be useful to distinguish between different digits?\nPersistence homology with Vietoris-Rips filtration won’t be of much help: all digits are connected, so the 0-persistence is useless; for the 1-dimensional persistence,\n\n1, 3, 5, 7 do not contain holes;\n2 and 4 sometimes contain one hole (depending on the way you write it);\n0, 6, 9 contain one hole each;\n8 contains two holes.\n\nWhat if we starting chopping the digits with sublevels of some functions? The excentricity function is able to highlight edges. Doing a sublevel filtration with the excentricity function will permit us to separate digits by the amount of edges they have. So 1 and 3 and 7, for example, will have different persistence diagrams.\n\n6.3.1 From square matrices to points in the plane\nIn order to calculate the excentricity, we need to convert the “image digits” (28x28 matrices) to points in \\(\\mathbb{R}^2\\) (matrices with 2 columns, one for each dimension, which we will call pointclouds). A simple function can do that:\n\nfunction img_to_points(img, threshold = 0.3)\n    ids = findall(x -&gt; x &gt;= threshold, img)\n    pts = getindex.(ids, [1 2])\nend;\n\nNotice that we had to define a threshold: coordinates with values less than the threshold are not considered.\nLet’s also define a function to plot a digit:\n\nfunction plot_digit(fig, values = :black)\n    pt = img_to_points(fig)\n    f = Figure();\n    ax = Makie.Axis(f[1, 1], autolimitaspect = 1, yreversed = true)\n    scatter!(ax, pt[:, 2], pt[:, 1]; markersize = 40, marker = :rect, color = values)\n    if values isa Vector{&lt;:Real}\n        Colorbar(f[1, 2])\n    end\n    f\nend;\n\nWe can see that it works as expected\n\nfig = figs[3]\nheatmap(fig)\n\n\n\n\nbut the image is flipped. This is easily fixed:\n\nheatmap(fig |&gt; rotr90)\n\n\n\n\n\n\n6.3.2 Excentricity\nGetting into details: the excentricity of a metric space \\((X, d)\\) is a measure of how far a point is from the “center”. It is defined as follows for each \\(x \\in X\\):\n\\[\ne(x) = \\sum_{y \\in X} \\frac{d(x, y)}{N}\n\\]\nwhere \\(N\\) is the amount of points of \\(X\\).\nDefine a function that takes a digit in \\(\\mathbb{R}^2\\) and return the excentricity as an 28x28 image\n\nfunction excentricity(fig)\n    pt = img_to_points(fig)\n    dists = pairwise(Euclidean(), pt')\n    excentricity = [mean(c) for c ∈ eachcol(dists)]\n    exc_matrix = zeros(28, 28)\n\n    for (row, (i, j)) ∈ enumerate(eachrow(pt))\n        exc_matrix[i, j] = excentricity[row]\n    end\n\n    return exc_matrix\nend;\n\nWe store all the excentricities in the excs vector\n\nexcs = excentricity.(figs);\n\nand plot a digit with it’s corresponding excentricity\n\ni = 5\nfig = figs[i]\nexc = excs[i]\nheatmap(exc |&gt; rotr90)\n\n\n\n\nLooks good! Time to chop it.\n\n\n6.3.3 Persistence images\nNow we calculate all the persistence diagrams using sublevel filtration. This can take some seconds.\n\npds = map(excs) do ex\n    m = maximum(ex)\n    ex = m .- ex\n    ripserer(Cubical(ex), cutoff = 0.5)\nend;\n\nWe check the first one\n\npd = pds[i]\npd |&gt; barcode\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare it with the corresponding heatmap above. There are 3 main edges (and one really small one). It seems ok!\nNow we create the persistence images of all these barcodes in dimension 0 and 1. We pass the entire collection of barcodes to the PersistenceImage function, and it will ensure that all of them are comparable (ie. are on the same grid).\n\npds_0 = pds .|&gt; first\npds_1 = pds .|&gt; last\nimgs_0 = PersistenceImage(pds_0; sigma = 1, size = 8)\nimgs_1 = PersistenceImage(pds_1; sigma = 1, size = 8);\n\nThe persistence images look ok too:\n\nPlots.plot(\n    barcode(pds[i])\n    , Plots.plot(pds[i]; persistence = true)\n    , Plots.heatmap(imgs_0(pds[i][1]); aspect_ratio=1)\n    ,  Plots.heatmap(imgs_1(pds[i][2]); aspect_ratio=1)\n    , layout = (2, 2)\n    )\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\nTop left: the barcode of a digit with respect to sublevels using the excentricity function. Top right: the corresponding persistence diagram. Bottom: 0 and 1 dimensional persistence images. They create a pixelated view of the persistence diagram, using a gaussian blur."
  },
  {
    "objectID": "digits.html#fitting-a-model",
    "href": "digits.html#fitting-a-model",
    "title": "6  Classifying hand-written digits",
    "section": "6.4 Fitting a model",
    "text": "6.4 Fitting a model\nIn order to use these persistence images in a machine learning model, we first need to vectorize them, that is, we need to transform them into a vector. Machine learning models love vectors! The easist way is to just concatenate the persistence images as follows:\n\nfunction concatenate_pds(imgs_0, pds_0, imgs_1, pds_1)\n    persims = [\n        [vec(imgs_0(pds_0[i])); vec(imgs_1(pds_1[i])) ] for i in 1:length(pds)\n        ]\n\n    X = reduce(hcat, persims)'\n    X\nend\n\nX = concatenate_pds(imgs_0, pds_0, imgs_1, pds_1)\ny = mnist_labels .|&gt; string;\n\nWe can see that X is a matrix with 8000 rows (the amount of digits) and 128 columns (the persistence images concatenated).\n\nX\n\n8000×128 adjoint(::Matrix{Float64}) with eltype Float64:\n 1.98864e-5   0.000274617  0.011089     …  0.0          0.0\n 0.019284     0.0178802    0.00552416      1.59632e-9   2.00159e-10\n 0.0048054    0.0163726    0.0411888       0.0          0.0\n 9.82746e-5   0.00783213   0.0263377       0.0          0.0\n 2.4359e-5    6.51723e-5   5.30449e-5      1.65099e-5   5.22603e-8\n 0.00482624   0.0214224    0.0401356    …  8.67968e-7   5.78855e-9\n 0.000612967  0.013268     0.0206519       0.0          0.0\n 0.00938679   0.00964761   0.00741125      0.0          0.0\n 0.000478196  0.0123035    0.0207728       0.0          0.0\n 7.34698e-5   0.00175636   0.0029649       0.0          0.0\n 0.0109329    0.0144404    0.0111531    …  0.0          0.0\n 0.000307403  0.00169679   0.00196591      0.0          0.0\n 0.00330491   0.00202755   0.00236959      0.0          0.0\n ⋮                                      ⋱               \n 0.000395629  0.00873113   0.0137106       0.0          0.0\n 0.00374747   0.00130186   3.58471e-5      0.0          0.0\n 0.00330984   0.00307851   0.00171305   …  0.000171869  2.57942e-6\n 0.00116416   0.00134763   0.000478334     0.0          0.0\n 0.000260679  0.00128307   0.00132153      0.0          0.0\n 0.000379909  0.0120085    0.0225162       0.0          0.0\n 0.000276005  0.000579846  0.000321933     0.0          0.0\n 5.10415e-5   0.000313229  0.000378251  …  3.09816e-5   2.24431e-7\n 0.000125361  0.00866741   0.0263348       0.0          0.0\n 0.00158265   0.00205563   0.000739224     0.0          0.0\n 0.00467303   0.0107131    0.0185538       0.0          0.0\n 0.0126304    0.0327978    0.0489344       0.0          0.0\n\n\nIt was also important to convert the mnist_labels to strings, because we want to classify the digits (and not do a regression on them).\nWe now have a vector for each image. What can we do? We need a model that takes a large vector of numbers and try to predict the digit. Neural networks are excellent in finding non-linear relations on vectors. Let’s try one!\nCreate the layers\n\nusing Flux, ProgressMeter\nfunction nn_model(X)\n  model = Chain(\n      Dense(size(X)[2] =&gt; 64)\n      ,Dense(64 =&gt; 10)\n  )\nend\n\nmodel = nn_model(X)\n\n\nChain(\n  Dense(128 =&gt; 64),                     # 8_256 parameters\n  Dense(64 =&gt; 10),                      # 650 parameters\n)                   # Total: 4 arrays, 8_906 parameters, 35.039 KiB.\n\n\n\nthe loader\n\ntarget = Flux.onehotbatch(y, 0:9 .|&gt; string)\nloader = Flux.DataLoader((X' .|&gt; Float32, target), batchsize=32, shuffle=true);\n\nthe optimiser\n\noptim = Flux.setup(Flux.Adam(0.01), model);\n\nand train it\n\n@showprogress for epoch in 1:100\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend;\n\nProgress:   2%|▉                                        |  ETA: 0:10:31\n\n\nProgress:  14%|█████▊                                   |  ETA: 0:01:24\n\n\nProgress:  24%|█████████▉                               |  ETA: 0:00:44\n\n\nProgress:  28%|███████████▌                             |  ETA: 0:00:36\n\n\nProgress:  32%|█████████████▏                           |  ETA: 0:00:30\n\n\nProgress:  35%|██████████████▍                          |  ETA: 0:00:27\n\n\nProgress:  38%|███████████████▋                         |  ETA: 0:00:24\n\n\nProgress:  41%|████████████████▊                        |  ETA: 0:00:21\n\n\nProgress:  45%|██████████████████▌                      |  ETA: 0:00:18\n\n\nProgress:  48%|███████████████████▋                     |  ETA: 0:00:16\n\n\nProgress:  52%|█████████████████████▍                   |  ETA: 0:00:14\n\n\nProgress:  56%|███████████████████████                  |  ETA: 0:00:12\n\n\nProgress:  60%|████████████████████████▋                |  ETA: 0:00:10\n\n\nProgress:  64%|██████████████████████████▎              |  ETA: 0:00:09\n\n\nProgress:  67%|███████████████████████████▌             |  ETA: 0:00:08\n\n\nProgress:  71%|█████████████████████████████▏           |  ETA: 0:00:06\n\n\nProgress:  75%|██████████████████████████████▊          |  ETA: 0:00:05\n\n\nProgress:  79%|████████████████████████████████▍        |  ETA: 0:00:04\n\n\nProgress:  82%|█████████████████████████████████▋       |  ETA: 0:00:03\n\n\nProgress:  86%|███████████████████████████████████▎     |  ETA: 0:00:03\n\n\nProgress:  90%|████████████████████████████████████▉    |  ETA: 0:00:02\n\n\nProgress:  94%|██████████████████████████████████████▌  |  ETA: 0:00:01\n\n\nProgress:  97%|███████████████████████████████████████▊ |  ETA: 0:00:01\n\n\nProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\n\n\n\n\n\nThe predictions can be made with\n\npred_y = model(X' .|&gt; Float32)\npred_y = Flux.onecold(pred_y, 0:9 .|&gt; string);\n\nAnd the accuracy\n\naccuracy = sum(pred_y .== y) / length(y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the train set was $accuracy %!\")\n\nThe accuracy on the train set was 70.89 %!\n\n\nNot bad, taking into account that we only used the excentricity sublevel filtration.\nThe confusion matrix is the following:\n\ntbl = freqtable(y, pred_y)\n\n10×10 Named Matrix{Int64}\nDim1 ╲ Dim2 │   0    1    2    3    4    5    6    7    8    9\n────────────┼─────────────────────────────────────────────────\n0           │ 744    0   15   10    2    3    4    2    7    8\n1           │   3  869    7    4    3   10    0   10    0    0\n2           │  28   15  434   43   74   51   13   29   71   30\n3           │  20   34   31  517   18  113    7   52   14    5\n4           │   1    2   68    7  626   17    1   64    8    5\n5           │  11   39   42  165    6  371   11   41    3   13\n6           │  22    1   11    8    7    5  520   36    9  179\n7           │   1   40   36   51  123   61   47  491    2    1\n8           │  17    2   56    7   11    1    2    0  644   13\n9           │  15    1   22   15   13   13  188   66    7  455\n\n\nCalculating the proportion of prediction for each digit, we get\n\nround2(x) = round(100*x, digits = 1)\n\nfunction prop_table(y1, y2)\n    tbl = freqtable(y1, y2)\n    tbl_prop = prop(tbl, margins = 1) .|&gt; round2\n    tbl_prop\nend\n\nprop_table(y, pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 93.6   0.0   1.9   1.3   0.3   0.4   0.5   0.3   0.9   1.0\n1           │  0.3  95.9   0.8   0.4   0.3   1.1   0.0   1.1   0.0   0.0\n2           │  3.6   1.9  55.1   5.5   9.4   6.5   1.6   3.7   9.0   3.8\n3           │  2.5   4.2   3.8  63.7   2.2  13.9   0.9   6.4   1.7   0.6\n4           │  0.1   0.3   8.5   0.9  78.3   2.1   0.1   8.0   1.0   0.6\n5           │  1.6   5.6   6.0  23.5   0.9  52.8   1.6   5.8   0.4   1.9\n6           │  2.8   0.1   1.4   1.0   0.9   0.6  65.2   4.5   1.1  22.4\n7           │  0.1   4.7   4.2   6.0  14.4   7.2   5.5  57.6   0.2   0.1\n8           │  2.3   0.3   7.4   0.9   1.5   0.1   0.3   0.0  85.5   1.7\n9           │  1.9   0.1   2.8   1.9   1.6   1.6  23.6   8.3   0.9  57.2\n\n\n!!!We see that the majority of 9 were classified as 6; the majority of 5 were wrong too. The only well predicted digits (&gt;90%) were 0 and 1.\n!!! fazer dictionary aqui pra pegar os piores erros\n!!! usar dataframes pra criar tabela de maiores erros\n!!! passar pra rede também a excentricidade total e densidade? dividir em 10 blocos?\n\n6.4.1 The perils of isometric spaces\nHow to separate “6” and “9”? They are isometric! For some people, “2” and “5” are also isometric (just mirror on the x-axis). Functions that only “see” the metric (like the excentricity) will never be able to separate these digits. In digits, the position of the features is important, so let’s add more slicing filtrations to our arsenal.\nTo avoid writing all the above code-blocks again, we encapsulate the whole process into a function\n\nfunction whole_process(\n    mnist_digits, mnist_labels, f\n    ; imgs_0 = nothing, imgs_1 = nothing\n    , dim_max = 1, sigma = 1, size_persistence_image = 8\n    )\n    figs = [mnist_digits[:, :, i]' |&gt; Matrix for i ∈ 1:size(mnist_digits)[3]]\n\n    excs = f.(figs);\n\n    pds = map(excs) do ex\n        m = maximum(ex)\n        ex = m .- ex\n        ripserer(Cubical(ex), cutoff = 0.5, dim_max = dim_max)\n    end;\n\n    pds_0 = pds .|&gt; first\n    pds_1 = pds .|&gt; last\n\n    if isnothing(imgs_0) \n        imgs_0 = PersistenceImage(pds_0; sigma = sigma, size = size_persistence_image) \n    end\n    if isnothing(imgs_1) \n        imgs_1 = PersistenceImage(pds_1; sigma = sigma, size = size_persistence_image) \n    end\n\n    persims = [\n    [vec(imgs_0(pds_0[i])); vec(imgs_1(pds_1[i])) ] for i in eachindex(pds)\n    ]\n\n    X = reduce(hcat, persims)'\n    y = mnist_labels .|&gt; string\n\n    return X, y, pds_0, pds_1, imgs_0, imgs_1\nend\n\nwhole_process (generic function with 1 method)\n\n\nWe now create the sideways filtrations: from the side and from above.\n\nset_value(x, threshold = 0.5, value = 0) = x ≥ threshold ? value : 0\n\nfunction filtration_sideways(fig; axis = 1, invert = false)\n\n  fig2 = copy(fig)\n  if axis == 2 fig2 = fig2' |&gt; Matrix end\n\n  for i ∈ 1:28\n    if invert k = 29 - i else k = i end\n    fig2[i, :] .= set_value.(fig2[i, :], 0.5, k)\n  end\n\n  fig2\n\nend;\n\nand calculate all 4 persistence diagrams. Warning: this can take a few seconds if you are using 60000 digits!\n\nfs = [\n    x -&gt; filtration_sideways(x, axis = 1, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = true)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = true)\n]\n\nret = @showprogress map(fs) do f\n    whole_process(\n        mnist_digits, mnist_labels, f\n        ,size_persistence_image = 8\n    )\nend;\n\nProgress:  50%|████████████████████▌                    |  ETA: 0:00:09\n\n\nProgress: 100%|█████████████████████████████████████████| Time: 0:00:13\n\n\nWe concatenate all the vectors\n\nX_list = ret .|&gt; first\nX_all = hcat(X, X_list...);\n\nand try again with a new model:\n\nmodel = nn_model(X_all)\n\ntarget = Flux.onehotbatch(y, 0:9 .|&gt; string)\nloader = Flux.DataLoader((X_all' .|&gt; Float32, target), batchsize=64, shuffle=true);\n\noptim = Flux.setup(Flux.Adam(0.01), model)\n\n@showprogress for epoch in 1:50\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend;\n\nProgress:   4%|█▋                                       |  ETA: 0:00:09\n\n\nProgress:   6%|██▌                                      |  ETA: 0:00:08\n\n\nProgress:   8%|███▎                                     |  ETA: 0:00:07\n\n\nProgress:  12%|████▉                                    |  ETA: 0:00:06\n\n\nProgress:  16%|██████▌                                  |  ETA: 0:00:05\n\n\nProgress:  20%|████████▎                                |  ETA: 0:00:04\n\n\nProgress:  24%|█████████▉                               |  ETA: 0:00:04\n\n\nProgress:  28%|███████████▌                             |  ETA: 0:00:04\n\n\nProgress:  32%|█████████████▏                           |  ETA: 0:00:03\n\n\nProgress:  36%|██████████████▊                          |  ETA: 0:00:03\n\n\nProgress:  40%|████████████████▍                        |  ETA: 0:00:03\n\n\nProgress:  44%|██████████████████                       |  ETA: 0:00:03\n\n\nProgress:  48%|███████████████████▋                     |  ETA: 0:00:02\n\n\nProgress:  52%|█████████████████████▍                   |  ETA: 0:00:02\n\n\nProgress:  56%|███████████████████████                  |  ETA: 0:00:02\n\n\nProgress:  60%|████████████████████████▋                |  ETA: 0:00:02\n\n\nProgress:  64%|██████████████████████████▎              |  ETA: 0:00:02\n\n\nProgress:  68%|███████████████████████████▉             |  ETA: 0:00:01\n\n\nProgress:  72%|█████████████████████████████▌           |  ETA: 0:00:01\n\n\nProgress:  76%|███████████████████████████████▏         |  ETA: 0:00:01\n\n\nProgress:  80%|████████████████████████████████▊        |  ETA: 0:00:01\n\n\nProgress:  84%|██████████████████████████████████▌      |  ETA: 0:00:01\n\n\nProgress:  88%|████████████████████████████████████▏    |  ETA: 0:00:01\n\n\nProgress:  92%|█████████████████████████████████████▊   |  ETA: 0:00:00\n\n\nProgress:  96%|███████████████████████████████████████▍ |  ETA: 0:00:00\n\n\nProgress: 100%|█████████████████████████████████████████| Time: 0:00:04\n\n\nNow we have\n\npred_y = model(X_all' .|&gt; Float32)\npred_y = Flux.onecold(pred_y, 0:9 .|&gt; string)\n\naccuracy = sum(pred_y .== y) / length(y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the train set was $accuracy %!\")\n\nThe accuracy on the train set was 95.45 %!\n\n\nwhich is certainly an improvement!\nThe proportional confusion matrix is\n\nprop_table(y, pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 98.5   0.0   0.5   0.0   0.3   0.4   0.0   0.0   0.1   0.3\n1           │  0.0  97.7   0.4   0.2   0.0   0.4   0.0   1.2   0.0   0.0\n2           │  0.0   0.5  84.9   0.3   0.3   8.8   0.0   5.1   0.3   0.0\n3           │  0.0   0.4   1.1  89.0   0.0   3.3   0.0   6.0   0.1   0.0\n4           │  0.0   0.1   0.0   0.0  99.0   0.0   0.0   0.5   0.0   0.4\n5           │  0.0   1.1   1.7   0.7   0.1  94.3   0.4   0.7   0.1   0.7\n6           │  0.0   0.3   0.4   0.0   0.5   0.6  97.9   0.1   0.0   0.3\n7           │  0.0   2.0   0.2   0.2   0.1   0.4   0.1  96.7   0.0   0.2\n8           │  0.0   0.1   0.4   0.0   0.4   0.1   0.0   0.1  98.5   0.3\n9           │  0.0   0.1   0.0   0.0   1.1   0.1   0.0   1.0   0.0  97.6"
  },
  {
    "objectID": "digits.html#learning-from-your-mistakes",
    "href": "digits.html#learning-from-your-mistakes",
    "title": "6  Classifying hand-written digits",
    "section": "6.5 Learning from your mistakes",
    "text": "6.5 Learning from your mistakes\nLet’s explore a bit where the model is making mistakes. Collect all the errors\n\nerrors = findall(pred_y .!= y)\n\n364-element Vector{Int64}:\n   25\n   26\n   29\n   55\n  101\n  110\n  121\n  133\n  139\n  169\n  173\n  181\n  190\n    ⋮\n 7834\n 7843\n 7861\n 7868\n 7880\n 7898\n 7899\n 7910\n 7917\n 7950\n 7955\n 7977\n\n\nand plot the first 3\n\ni = errors[1]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 2 but it was a 1\n\n\n\n\n\n\ni = errors[2]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 5 but it was a 2\n\n\n\n\n\n\ni = errors[3]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 5 but it was a 2\n\n\n\n\n\nWe can make a mosaic with the first 100 errors\n\nn = 10\nfigs_plot = [figs[i] .|&gt; Gray for i in errors[1:n^2]]\nmosaicview(figs_plot, nrow = n, rowmajor = true)"
  },
  {
    "objectID": "digits.html#getting-new-data",
    "href": "digits.html#getting-new-data",
    "title": "6  Classifying hand-written digits",
    "section": "6.6 Getting new data",
    "text": "6.6 Getting new data\nNow we want to see if our model really learned something, or if it just repeated what he saw in the training data. To check data, we need to get new data and calculate the accuracy of the same model on this new data.\n\nn_test = 1_000\nnew_mnist_digits, new_mnist_labels = MLDatasets.MNIST(split=:test)[:];\nnew_mnist_digits = new_mnist_digits[:, :, 1:n_test]\nnew_mnist_labels = new_mnist_labels[1:n_test];\n\nand obtaning X and y to feed the model\n\nfs = [\n    x -&gt; excentricity(x)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = true)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = true)\n]\n\nret = @showprogress map(fs) do f\n    whole_process(\n        new_mnist_digits, new_mnist_labels, f\n        ,size_persistence_image = 8\n    )\nend;\n\nProgress:  40%|████████████████▍                        |  ETA: 0:00:02\n\n\nProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\n\n\nDefine our new X and y\n\nnew_X = ret .|&gt; first\nnew_X = hcat(new_X...)\nnew_y = ret[1][2]\n\nnew_pred_y = model(new_X' .|&gt; Float32)\nnew_pred_y = Flux.onecold(new_pred_y, 0:9 .|&gt; string);\n\nand calculate the accuracy:\n\naccuracy = sum(new_pred_y .== new_y) / length(new_y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the test data was $accuracy %!\")\n\nThe accuracy on the test data was 88.2 %!\n\n\nA bit less than the training set, but not so bad.\nLet’s check the confusion matrix\n\ntbl = prop_table(new_y, new_pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 91.8   0.0   0.0   0.0   3.5   0.0   0.0   0.0   4.7   0.0\n1           │  0.0  96.8   0.0   0.0   0.0   0.8   0.0   2.4   0.0   0.0\n2           │  0.0   0.0  81.9   2.6   0.9   8.6   0.0   3.4   2.6   0.0\n3           │  0.0   0.0   3.7  88.8   0.0   2.8   0.0   4.7   0.0   0.0\n4           │  0.0   0.0   0.9   0.0  94.5   1.8   0.0   0.0   0.9   1.8\n5           │  0.0   0.0  20.7   2.3   2.3  72.4   0.0   2.3   0.0   0.0\n6           │  0.0   0.0   0.0   0.0   3.4   5.7  89.7   0.0   1.1   0.0\n7           │  0.0   3.0   2.0   3.0   4.0   1.0   0.0  86.9   0.0   0.0\n8           │  1.1   0.0   6.7   0.0   0.0   0.0   0.0   0.0  89.9   2.2\n9           │  0.0   0.0   0.0   0.0   1.1   0.0   0.0   6.4   6.4  86.2"
  },
  {
    "objectID": "topology.html#the-notion-of-sameness-in-topology",
    "href": "topology.html#the-notion-of-sameness-in-topology",
    "title": "2  Topology",
    "section": "2.2 The notion of “sameness” in topology",
    "text": "2.2 The notion of “sameness” in topology\n\nDefinition 2.2 Given two topological spaces \\((X, \\tau)\\) and \\((Y, \\sigma)\\), a function \\(f: X \\to Y\\) is said to be continuous if the inverse image of any open set of \\(Y\\) is also an open set of \\(X\\). In symbols:\n\\[\n\\forall \\; V \\in \\sigma, \\; f^{-1}(V) \\in \\tau\n\\]\n\nCup and donut\nTopology only looks to open sets\nAdvantages?"
  },
  {
    "objectID": "persistence.html",
    "href": "persistence.html",
    "title": "5  Persistent homology",
    "section": "",
    "text": "“The essence of this architecture is movement synchronized towards a precise objective. We observe a fraction of the process, like hearing the vibration of a single string in an orchestra of supergiants. We know, but cannot grasp, that above and below, beyond the limits of perception or imagination, thousands and millions of simultaneous transformations are at work, interlinked like a musical score by mathematical counterpoint. It has been described as a symphony in geometry, but we lack the ears to hear it.”\n— Stanisław Lem, in “Solaris”"
  }
]