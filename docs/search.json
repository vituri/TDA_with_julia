[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "“The unexpected elation with which I had talked about mathematics had suddenly evaporated, and I sat beside him, feeling the weight of my own body, its unnecessary size. Outside of mathematics we had nothing to say to each other, and we both knew it. Then it occurred to me that the emotion with which I had spoken of the blessed role of mathematics on the voyage was a deception. I had been deceiving myself with the modesty, the serious heroism of the pilot who occupies himself, in the gaps of the nebulae, with theoretical studies of infinity. Hypocrisy. For what had it been, really? If a castaway, adrift for months at sea, has a thousand times counted the number of wood fibers that make up his raft, in order to keep sane, should he boast about it when he reaches land? That he had the tenacity to survive? And what of it? Who cared? Why should it matter to anyone how I had filled my poor brain those ten years, and why was that more important than how I had filled my stomach?”\n— Stanisław Lem, in “Return from the stars”"
  },
  {
    "objectID": "topology.html#what-is-topology",
    "href": "topology.html#what-is-topology",
    "title": "2  Topology",
    "section": "2.1 What is topology?",
    "text": "2.1 What is topology?\nTopology is the study of topological spaces and its properties. Unfortunately, this is not enough to close this chapter.\nA topological space can be thought of as a set with little “scales” called open sets. Each of these open sets represent the notion of local continuity or connectivity that we sense when we look at an object: the feeling that it is just “one single piece of a thing”, like the scales in a fish. We can stretch these scales, but can’t rip them apart. In a sense, topological spaces are elastic. Because of that, there is an old joke that a topologist can’t distinguish between a cup and a donut.\n\n\n\nA continuous transformation of a cup to a donut. A true topologist will try to bite all these cups. Source: Wikipedia.1\n\n\nMore formally,\n\nDefinition 2.1 A topological space is a pair \\((X, \\tau)\\) where\n\n\\(X\\) is a set;\n\\(\\tau\\) is a set of subsets of \\(X\\), that is: each \\(U \\in \\tau\\) is a set \\(U \\subseteq X\\).\n\nThe set \\(\\tau\\) has the following properties:\n\nthe intersection of a finite number of open sets is also an open set;\nthe union of an arbitrary (even infinity) number of open sets is also an open set;\nthe \\(\\emptyset\\) and \\(X\\) are open sets.\n\nThe set \\(\\tau\\) is called a topology on \\(X\\).\n\nWe often omit \\(\\tau\\) and simply write “a topological space \\(X\\)”. Sometimes we omit the “topological” too. We love omiting!\n\nDefinition 2.2 The set of all subsets (or “parts”) of \\(X\\) is denoted by \\(P(X)\\), that is,\n\\[\nP(X) = \\{ U \\; | \\; U \\subseteq X \\}.\n\\]\n\nGiven a set \\(X\\), there is a easy way to generate a topology on it:\n\nDefinition 2.3 Let \\(X\\) be a set and \\(S \\subseteq P(X)\\). Define \\(\\tau\\) \\(\\{\\emptyset, X\\}\\) united with finite intersections and arbitrary unions of elements of \\(S\\). The pair \\((X, \\tau)\\) is a topological space, and \\(\\tau\\) is said to be “generated by \\(S\\)”. We also say that \\(S\\) is a generator set for \\(\\tau\\).\n\nDon’t let the abstractions hurt you! Whenever you see a new definition or theorem, think of an example of objects that fit in it. One very useful example is the following:\n\nExample 2.1 The standard topology on the set \\(\\mathbb{R}\\) of real numbers is generated by the open intervals \\((a, b) \\subseteq \\mathbb{R}\\).\n\nIn the above case, an open set is a set in which one can always “walk a little more without reaching the end of the set”. More precisely, given \\(x \\in (a, b)\\) there is always a small enough \\(\\epsilon &gt; 0\\) such the interval \\((x - \\epsilon, x + \\epsilon)\\) is contained in \\((a, b)\\).\n!![fora de lugar] When an open set \\(U\\) cannot be written as the disjoint union of two open sets, we say that \\(U\\) is connected. These connected open sets give a notion of “being a single piece”."
  },
  {
    "objectID": "topology.html#continuity",
    "href": "topology.html#continuity",
    "title": "2  Topology",
    "section": "2.2 Continuity",
    "text": "2.2 Continuity\nWe learn in Calculus that a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is continuous when we can draw its graph “without lifting the pencil from the paper”. Formalizing this notion of continuity took centuries and made many mathematicians go crazy.2 Let’s try to formalize what is “to lift the pencil from the paper” with the following example:\n\n\n\nAn example of a discontinuous function. Source: byjus.com3\n\n\nWe notice that no matter how close to 3 we choose some \\(x \\in \\mathbb{R}\\), its image \\(f(x)\\) will end up being far away from \\(f(3)\\). A bit more precisely, there is a small interval around \\(f(3)\\), say \\((f(3) - \\epsilon, f(3) + \\epsilon)\\), such that no matter how small we chose an interval around \\(3\\), say \\((3 - \\delta, 3 + \\delta)\\) we will have that\n\\[\nf(x) \\notin (f(3) - \\epsilon, f(3) + \\epsilon), \\; \\text{for some } x \\in (3 - \\delta, 3 + \\delta).\n\\]\n!![checar se faz sentido] This is equivalent to the fact that the inverse image of \\((f(3) - \\epsilon, f(3) + \\epsilon)\\) is not an open set: if it were an open set, we could always “walk a little” on this interval, and still be sent by \\(f\\) to \\((f(3) - \\epsilon, f(3) + \\epsilon)\\).\nInspired by that, we define the following in the much more general context of topological spaces:\n\nDefinition 2.4 Given two topological spaces \\((X, \\tau)\\) and \\((Y, \\sigma)\\), a function \\(f: X \\to Y\\) is said to be continuous if the inverse image of any open set of \\(Y\\) is also an open set of \\(X\\). In symbols:\n\\[\n\\forall \\; V \\in \\sigma, \\; f^{-1}(V) \\in \\tau.\n\\]\n\n\nRemark. Negating what we obtained in our example above leads us to the definition of continuity of real functions, and to the most terrifying single line of math of first-year students: a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is continuous if for every point \\(a \\in \\mathbb{R}\\) we have\n\\[\n\\forall \\epsilon &gt; 0, \\exists \\delta &gt; 0 \\text{ such that } x \\in (a - \\delta, a + \\delta) \\text{ implies } f(x) \\in (f(a) - \\epsilon, f(a) + \\epsilon),\n\\]\nwhich is equivalent to \\(\\lim_{x \\to a} f(x) = a\\)."
  },
  {
    "objectID": "topology.html#the-notion-of-sameness-in-topology",
    "href": "topology.html#the-notion-of-sameness-in-topology",
    "title": "2  Topology",
    "section": "2.3 The notion of “sameness” in topology",
    "text": "2.3 The notion of “sameness” in topology\nLet’s digress a little.\nA topological space is completely define by a set \\(X\\) and its open sets \\(\\tau\\). If we just “rename” the elements of \\(X\\), then it is intuitive that this new topological space is as similar to \\(X\\) as possible; in fact, they are indistinguishable from each other: every topological property (ie: properties related to open sets) that the first has, the second also has.\nRenaming points is just a informally way to say “bijection of sets”. So, two topological spaces \\(X\\) and \\(Y\\) are “the same thing, topologically speaking” if there is a bijection \\(f: X \\to Y\\) such that \\(U \\subseteq X\\) is open if and only if \\(f(X) \\subseteq Y\\) is open. More precisely,\n\nDefinition 2.5 We say that two topological spaces \\(X\\) and \\(Y\\) are homeomorphic if there exists a continuous bijection \\(f: X \\to Y\\) such that \\(f^{-1}\\) is also continuous. In this case, \\(f\\) is called an homeomorphism."
  },
  {
    "objectID": "topology.html#footnotes",
    "href": "topology.html#footnotes",
    "title": "2  Topology",
    "section": "",
    "text": "https://commons.wikimedia.org/wiki/File:Topology_joke.jpg↩︎\nI just invented this. Maybe some of them really got crazy, who knows?↩︎\nhttps://byjus.com/maths/discontinuity/↩︎"
  },
  {
    "objectID": "simplicial.html",
    "href": "simplicial.html",
    "title": "3  Simplicial complexes",
    "section": "",
    "text": "“Perfection does not exist; to comprehend it is the triumph of human intelligence; to desire to possess it, the most dangerous of follies.”\n— Alfred de Musset, in “The confession of a child of the century”"
  },
  {
    "objectID": "homology.html",
    "href": "homology.html",
    "title": "4  Simplicial homology",
    "section": "",
    "text": "Introduzir homologia como modo de contar buracos. Ideia intuitiva, depois formalização"
  },
  {
    "objectID": "persistence.html",
    "href": "persistence.html",
    "title": "5  Persistent homology",
    "section": "",
    "text": "“The essence of this architecture is movement synchronized towards a precise objective. We observe a fraction of the process, like hearing the vibration of a single string in an orchestra of supergiants. We know, but cannot grasp, that above and below, beyond the limits of perception or imagination, thousands and millions of simultaneous transformations are at work, interlinked like a musical score by mathematical counterpoint. It has been described as a symphony in geometry, but we lack the ears to hear it.”\n— Stanisław Lem, in “Solaris”"
  },
  {
    "objectID": "digits.html#loading-packages",
    "href": "digits.html#loading-packages",
    "title": "6  Classifying hand-written digits",
    "section": "6.1 Loading packages",
    "text": "6.1 Loading packages\n\nimport MLDatasets\nusing Images, Makie, CairoMakie\nusing Distances\nusing Ripserer, PersistenceDiagrams\nusing StatsBase: mean\nimport Plots;\nusing DataFrames, FreqTables, PrettyTables\nusing Flux, ProgressMeter"
  },
  {
    "objectID": "digits.html#the-dataset",
    "href": "digits.html#the-dataset",
    "title": "6  Classifying hand-written digits",
    "section": "6.2 The dataset",
    "text": "6.2 The dataset\nMNIST is a dataset consisting of 70.000 hand-written digits. Each digit is a 28x28 grayscale image, that is: a 28x28 matrix of values from 0 to 1. To get this dataset, run\n\nn_train = 10_000\nmnist_digits, mnist_labels = MLDatasets.MNIST(split=:train)[:];\nmnist_digits = mnist_digits[:, :, 1:n_train]\nmnist_labels = mnist_labels[1:n_train];\n\nIf the console asks you to download some data, just press y.\nNotice that we only get the first n_train images so this notebook doesn’t take too much time to run. You can increase n_train to 60000 if you like to live dangerously and have enough RAM memory.\nNext, we transpose the digits and save them in a vector\n\nfigs = [mnist_digits[:, :, i]' |&gt; Matrix for i ∈ 1:size(mnist_digits)[3]];\n\nThe first digit, for example, is the following matrix:\n\nfigs[1]\n\n28×28 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.498039  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.25098   0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n ⋮                             ⋮         ⋱                 ⋮         \n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.215686  0.67451      0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.533333  0.992157     0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0       …  0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0       0.0          0.0       0.0  0.0  0.0  0.0\n\n\nWe can see a mosaic with the first 10^2 digits\n\nn = 10\nfigs_plot = [fig .|&gt; Gray for fig in figs[1:n^2]]\nmosaicview(figs_plot, nrow = n, rowmajor = true)"
  },
  {
    "objectID": "digits.html#preparing-for-war",
    "href": "digits.html#preparing-for-war",
    "title": "6  Classifying hand-written digits",
    "section": "6.3 Preparing for war",
    "text": "6.3 Preparing for war\nWhat topological tools can be useful to distinguish between different digits?\nPersistence homology with Vietoris-Rips filtration won’t be of much help: all digits are connected, so the 0-persistence is useless; for the 1-dimensional persistence,\n\n1, 3, 5, 7 do not contain holes;\n2 and 4 sometimes contain one hole (depending on the way you write it);\n0, 6, 9 contain one hole each;\n8 contains two holes.\n\nWhat if we starting chopping the digits with sublevels of some functions? The excentricity function is able to highlight edges. Doing a sublevel filtration with the excentricity function will permit us to separate digits by the amount of edges they have. So 1 and 3 and 7, for example, will have different persistence diagrams.\n\n6.3.1 From square matrices to points in the plane\nIn order to calculate the excentricity, we need to convert the “image digits” (28x28 matrices) to points in \\(\\mathbb{R}^2\\) (matrices with 2 columns, one for each dimension, which we will call pointclouds). A simple function can do that:\n\nfunction img_to_points(img, threshold = 0.3)\n    ids = findall(x -&gt; x &gt;= threshold, img)\n    pts = getindex.(ids, [1 2])\nend;\n\nNotice that we had to define a threshold: coordinates with values less than the threshold are not considered.\nLet’s also define a function to plot a digit:\n\nfunction plot_digit(fig, values = :black)\n    pt = img_to_points(fig)\n    f = Figure();\n    ax = Makie.Axis(f[1, 1], autolimitaspect = 1, yreversed = true)\n    scatter!(ax, pt[:, 2], pt[:, 1]; markersize = 40, marker = :rect, color = values)\n    if values isa Vector{&lt;:Real}\n        Colorbar(f[1, 2])\n    end\n    f\nend;\n\nWe can see that it works as expected\n\nfig = figs[3]\nheatmap(fig)\n\n\n\n\nbut the image is flipped. This is easily fixed:\n\nheatmap(fig |&gt; rotr90)\n\n\n\n\n\n\n6.3.2 Excentricity\nGetting into details: the excentricity of a metric space \\((X, d)\\) is a measure of how far a point is from the “center”. It is defined as follows for each \\(x \\in X\\):\n\\[\ne(x) = \\sum_{y \\in X} \\frac{d(x, y)}{N}\n\\]\nwhere \\(N\\) is the amount of points of \\(X\\).\nDefine a function that takes a digit in \\(\\mathbb{R}^2\\) and return the excentricity as an 28x28 image\n\nfunction excentricity(fig)\n    pt = img_to_points(fig)\n    dists = pairwise(Euclidean(), pt')\n    excentricity = [mean(c) for c ∈ eachcol(dists)]\n    exc_matrix = zeros(28, 28)\n\n    for (row, (i, j)) ∈ enumerate(eachrow(pt))\n        exc_matrix[i, j] = excentricity[row]\n    end\n\n    return exc_matrix\nend;\n\nWe store all the excentricities in the excs vector\n\nexcs = excentricity.(figs);\n\nand plot a digit with it’s corresponding excentricity\n\ni = 5\nfig = figs[i]\nexc = excs[i]\nheatmap(exc |&gt; rotr90)\n\n\n\n\nLooks good! Time to chop it.\n\n\n6.3.3 Persistence images\nNow we calculate all the persistence diagrams using sublevel filtration. This can take some seconds. Julia is incredibly fast, but does not perform miracles (yet!).\n\npds = map(excs) do ex\n    m = maximum(ex)\n    ex = m .- ex\n    ripserer(Cubical(ex), cutoff = 0.5)\nend;\n\nWe check the first one\n\npd = pds[i]\npd |&gt; barcode\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare it with the corresponding heatmap above. There are 3 main edges (and one really small one). It seems ok!\nWe can see the “step-by-step” creation of these connected components in the following mosaic.\n\nr = range(minimum(exc), maximum(exc), length = 25) |&gt; reverse\nfigs_filtration = map(r) do v\n    replace(x -&gt; x ≤ v ? 0 : 1, exc) .|&gt; Gray\nend\n\nmosaicview(figs_filtration..., rowmajor = true, nrow = 5, npad = 20)\n\n\n\n\nNow we create the persistence images of all these barcodes in dimension 0 and 1. We pass the entire collection of barcodes to the PersistenceImage function, and it will ensure that all of them are comparable (ie. are on the same grid).\n\npds_0 = pds .|&gt; first\npds_1 = pds .|&gt; last\nimgs_0 = PersistenceImage(pds_0; sigma = 1, size = 8)\nimgs_1 = PersistenceImage(pds_1; sigma = 1, size = 8);\n\nThe persistence images look ok too:\n\nPlots.plot(\n    barcode(pds[i])\n    , Plots.plot(pds[i]; persistence = true)\n    , Plots.heatmap(imgs_0(pds[i][1]); aspect_ratio=1)\n    ,  Plots.heatmap(imgs_1(pds[i][2]); aspect_ratio=1)\n    , layout = (2, 2)\n    )\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\nTop left: the barcode of a digit with respect to sublevels using the excentricity function. Top right: the corresponding persistence diagram. Bottom: 0 and 1 dimensional persistence images. They create a pixelated view of the persistence diagram, using a gaussian blur."
  },
  {
    "objectID": "digits.html#fitting-a-model",
    "href": "digits.html#fitting-a-model",
    "title": "6  Classifying hand-written digits",
    "section": "6.4 Fitting a model",
    "text": "6.4 Fitting a model\nIn order to use these persistence images in a machine learning model, we first need to vectorize them, ie, transform them into a vector. Machine learning models love vectors! The easist way is to just concatenate the persistence images as follows:\n\nfunction concatenate_pds(imgs_0, pds_0, imgs_1, pds_1)\n    persims = [\n        [vec(imgs_0(pds_0[i])); vec(imgs_1(pds_1[i])) ] for i in 1:length(pds)\n        ]\n\n    X = reduce(hcat, persims)'\n    X\nend\n\nX = concatenate_pds(imgs_0, pds_0, imgs_1, pds_1)\ny = mnist_labels .|&gt; string;\n\nWe can see that X is a matrix with 10000 rows (the amount of digits) and 128 columns (the persistence images concatenated).\nIt was also important to convert the mnist_labels to strings, because we want to classify the digits (and not do a regression on them).\nWe now have a vector for each image. What can we do? We need a model that takes a large vector of numbers and try to predict the digit. Neural networks are excellent in finding non-linear relations on vectors. Let’s try one!\nCreate the layers\n\nfunction nn_model(X)\n  model = Chain(\n      Dense(size(X)[2] =&gt; 64)\n      ,Dense(64 =&gt; 10)\n  )\nend\n\nmodel = nn_model(X)\n\n\nChain(\n  Dense(128 =&gt; 64),                     # 8_256 parameters\n  Dense(64 =&gt; 10),                      # 650 parameters\n)                   # Total: 4 arrays, 8_906 parameters, 35.039 KiB.\n\n\n\nthe loader\n\ntarget = Flux.onehotbatch(y, 0:9 .|&gt; string)\nloader = Flux.DataLoader((X' .|&gt; Float32, target), batchsize=32, shuffle=true);\n\nthe optimiser\n\noptim = Flux.setup(Flux.Adam(0.01), model);\n\nand train it\n\n@showprogress for epoch in 1:100\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend;\n\nThe predictions can be made with\n\npred_y = model(X' .|&gt; Float32)\npred_y = Flux.onecold(pred_y, 0:9 .|&gt; string);\n\nAnd the accuracy\n\naccuracy = sum(pred_y .== y) / length(y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the train set was $accuracy %!\")\n\nThe accuracy on the train set was 70.3 %!\n\n\nNot bad, taking into account that we only used the excentricity sublevel filtration.\nThe confusion matrix is the following:\n\ntbl = freqtable(y, pred_y)\n\n10×10 Named Matrix{Int64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │  917     0    19     6     2    18    19     2    12     6\n1           │    0  1086     5     2     4     8     0    22     0     0\n2           │   20    26   523    34   124    50    20    78    82    34\n3           │    7    57    43   622    21   122    13   127    16     4\n4           │    0     2    64     4   773    11     2   109     8     7\n5           │    8    64    46   181     9   384    17   131     5    18\n6           │   15     4    10     4     6     5   739    70    10   151\n7           │    2    51    29    17   140    25    56   748     2     0\n8           │   13     5    72     8    25     0     6     7   776    32\n9           │   12     1    21     9    16    10   330   104    13   462\n\n\nCalculating the proportion of prediction for each digit, we get\n\nround2(x) = round(100*x, digits = 1)\n\nfunction prop_table(y1, y2)\n    tbl = freqtable(y1, y2)\n    tbl_prop = prop(tbl, margins = 1) .|&gt; round2\n    tbl_prop\nend\n\ntbl_p = prop_table(y, pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 91.6   0.0   1.9   0.6   0.2   1.8   1.9   0.2   1.2   0.6\n1           │  0.0  96.4   0.4   0.2   0.4   0.7   0.0   2.0   0.0   0.0\n2           │  2.0   2.6  52.8   3.4  12.5   5.0   2.0   7.9   8.3   3.4\n3           │  0.7   5.5   4.2  60.3   2.0  11.8   1.3  12.3   1.6   0.4\n4           │  0.0   0.2   6.5   0.4  78.9   1.1   0.2  11.1   0.8   0.7\n5           │  0.9   7.4   5.3  21.0   1.0  44.5   2.0  15.2   0.6   2.1\n6           │  1.5   0.4   1.0   0.4   0.6   0.5  72.9   6.9   1.0  14.9\n7           │  0.2   4.8   2.7   1.6  13.1   2.3   5.2  69.9   0.2   0.0\n8           │  1.4   0.5   7.6   0.8   2.6   0.0   0.6   0.7  82.2   3.4\n9           │  1.2   0.1   2.1   0.9   1.6   1.0  33.7  10.6   1.3  47.2\n\n\nWe see that the biggest errors are the following:\n\nfunction top_errors(tbl_p)\n    df = DataFrame(\n        Digit = Integer[]\n        , Prediction = Integer[]\n        , Percentage = Float64[]\n        )\n    \n    for i = eachindex(IndexCartesian(), tbl_p)\n        push!(df, (i[1]-1, i[2]-1, tbl_p[i]))    \n    end   \n   \n    filter!(row -&gt; row.Digit != row.Prediction, df)\n    sort!(df, :Percentage, rev = true)\n    df[1:10, :]    \nend\n\ndf_errors = top_errors(tbl_p)\ndf_errors |&gt; pretty_table\n\n┌─────────┬────────────┬────────────┐\n│   Digit │ Prediction │ Percentage │\n│ Integer │    Integer │    Float64 │\n├─────────┼────────────┼────────────┤\n│       9 │          6 │       33.7 │\n│       5 │          3 │       21.0 │\n│       5 │          7 │       15.2 │\n│       6 │          9 │       14.9 │\n│       7 │          4 │       13.1 │\n│       2 │          4 │       12.5 │\n│       3 │          7 │       12.3 │\n│       3 │          5 │       11.8 │\n│       4 │          7 │       11.1 │\n│       9 │          7 │       10.6 │\n└─────────┴────────────┴────────────┘\n\n\n\n6.4.1 The perils of isometric spaces\nHow to separate “6” and “9”? They are isometric! For some people, “2” and “5” are also isometric (just mirror on the x-axis). Functions that only “see” the metric (like the excentricity) will never be able to separate these digits. In digits, the position of the features is important, so let’s add more slicing filtrations to our arsenal.\nTo avoid writing all the above code-blocks again, we encapsulate the whole process into a function\n\nfunction whole_process(\n    mnist_digits, mnist_labels, f\n    ; imgs_0 = nothing, imgs_1 = nothing\n    , dim_max = 1, sigma = 1, size_persistence_image = 8\n    )\n    figs = [mnist_digits[:, :, i]' |&gt; Matrix for i ∈ 1:size(mnist_digits)[3]]\n\n    excs = f.(figs);\n\n    pds = map(excs) do ex\n        m = maximum(ex)\n        ex = m .- ex\n        ripserer(Cubical(ex), cutoff = 0.5, dim_max = dim_max)\n    end;\n\n    pds_0 = pds .|&gt; first\n    pds_1 = pds .|&gt; last\n\n    if isnothing(imgs_0) \n        imgs_0 = PersistenceImage(pds_0; sigma = sigma, size = size_persistence_image) \n    end\n    if isnothing(imgs_1) \n        imgs_1 = PersistenceImage(pds_1; sigma = sigma, size = size_persistence_image) \n    end\n\n    persims = [\n    [vec(imgs_0(pds_0[i])); vec(imgs_1(pds_1[i])) ] for i in eachindex(pds)\n    ]\n\n    X = reduce(hcat, persims)'\n    y = mnist_labels .|&gt; string\n\n    return X, y, pds_0, pds_1, imgs_0, imgs_1\nend;\n\nWe now create the sideways filtrations: from the side and from above.\n\nset_value(x, threshold = 0.5, value = 0) = x ≥ threshold ? value : 0\n\nfunction filtration_sideways(fig; axis = 1, invert = false)\n\n  fig2 = copy(fig)\n  if axis == 2 fig2 = fig2' |&gt; Matrix end\n\n  for i ∈ 1:28\n    if invert k = 29 - i else k = i end\n    fig2[i, :] .= set_value.(fig2[i, :], 0.5, k)\n  end\n\n  fig2\n\nend;\n\nand calculate all 4 persistence diagrams. Warning: this can take a few seconds if you are using 60000 digits!\n\nfs = [\n    x -&gt; filtration_sideways(x, axis = 1, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = true)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = true)\n]\n\nret = @showprogress map(fs) do f\n    whole_process(\n        mnist_digits, mnist_labels, f\n        ,size_persistence_image = 8\n    )\nend;\n\nWe concatenate all the vectors\n\nX_list = ret .|&gt; first\nX_all = hcat(X, X_list...);\n\nand try again with a new model:\n\nmodel = nn_model(X_all)\n\ntarget = Flux.onehotbatch(y, 0:9 .|&gt; string)\nloader = Flux.DataLoader((X_all' .|&gt; Float32, target), batchsize=64, shuffle=true);\n\noptim = Flux.setup(Flux.Adam(0.01), model)\n\n@showprogress for epoch in 1:50\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend;\n\nNow we have\n\npred_y = model(X_all' .|&gt; Float32)\npred_y = Flux.onecold(pred_y, 0:9 .|&gt; string)\n\naccuracy = sum(pred_y .== y) / length(y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the train set was $accuracy %!\")\n\nThe accuracy on the train set was 95.1 %!\n\n\nwhich is certainly an improvement!\nThe proportional confusion matrix is\n\nprop_table(y, pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 99.3   0.0   0.1   0.1   0.0   0.0   0.3   0.0   0.2   0.0\n1           │  0.0  97.5   0.5   0.5   0.2   0.3   0.0   0.9   0.1   0.0\n2           │  0.3   0.5  85.4   2.9   0.1   6.9   1.6   1.9   0.4   0.0\n3           │  0.3   0.3   0.7  95.4   0.0   1.3   0.1   1.7   0.2   0.0\n4           │  0.1   0.0   0.0   0.0  98.9   0.0   0.4   0.3   0.1   0.2\n5           │  0.2   0.7   2.4   2.3   0.1  91.9   2.1   0.1   0.0   0.1\n6           │  0.0   0.2   0.1   0.0   0.1   0.0  99.6   0.0   0.0   0.0\n7           │  0.5   2.3   1.4   3.0   0.7   0.4   0.2  91.4   0.0   0.2\n8           │  0.1   0.1   0.5   0.0   1.3   0.1   0.8   0.0  96.8   0.2\n9           │  0.5   0.1   0.1   0.3   1.5   0.5   1.1   0.5   1.0  94.3"
  },
  {
    "objectID": "digits.html#learning-from-your-mistakes",
    "href": "digits.html#learning-from-your-mistakes",
    "title": "6  Classifying hand-written digits",
    "section": "6.5 Learning from your mistakes",
    "text": "6.5 Learning from your mistakes\nLet’s explore a bit where the model is making mistakes. Collect all the errors\n\nerrors = findall(pred_y .!= y);\n\nand plot the first 3\n\ni = errors[1]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 2 but it was a 1\n\n\n\n\n\n\ni = errors[2]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 5 but it was a 2\n\n\n\n\n\n\ni = errors[3]\nprintln(\"The model predicted a $(pred_y[i]) but it was a $(y[i])\")\nplot_digit(figs[i])\n\nThe model predicted a 5 but it was a 2\n\n\n\n\n\nWe can make a mosaic with the first 100 errors\n\nn = 10\nfigs_plot = [figs[i] .|&gt; Gray for i in errors[1:n^2]]\nmosaicview(figs_plot, nrow = n, rowmajor = true)\n\n\n\n\nMany of these digits are really ugly! This makes them hard to classify with our sublevel filtrations. Some other functions could be explored."
  },
  {
    "objectID": "digits.html#getting-new-data",
    "href": "digits.html#getting-new-data",
    "title": "6  Classifying hand-written digits",
    "section": "6.6 Getting new data",
    "text": "6.6 Getting new data\nNow we want to see if our model really learned something, or if it just repeated what he saw in the training data. To check data, we need to get new data and calculate the accuracy of the same model on this new data.\n\nn_test = 5_000\nnew_mnist_digits, new_mnist_labels = MLDatasets.MNIST(split=:test)[:];\nnew_mnist_digits = new_mnist_digits[:, :, 1:n_test]\nnew_mnist_labels = new_mnist_labels[1:n_test];\n\nand obtaning X and y to feed the model\n\nfs = [\n    x -&gt; excentricity(x)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = false)\n    ,x -&gt; filtration_sideways(x, axis = 1, invert = true)\n    ,x -&gt; filtration_sideways(x, axis = 2, invert = true)\n]\n\nret = @showprogress map(fs) do f\n    whole_process(\n        new_mnist_digits, new_mnist_labels, f\n        ,size_persistence_image = 8\n    )\nend;\n\nDefine our new X and y\n\nnew_X = ret .|&gt; first\nnew_X = hcat(new_X...)\nnew_y = ret[1][2]\n\nnew_pred_y = model(new_X' .|&gt; Float32)\nnew_pred_y = Flux.onecold(new_pred_y, 0:9 .|&gt; string);\n\nand calculate the accuracy:\n\naccuracy = sum(new_pred_y .== new_y) / length(new_y)\naccuracy = round(accuracy * 100, digits = 2)\nprintln(\"The accuracy on the test data was $accuracy %!\")\n\nThe accuracy on the test data was 87.1 %!\n\n\nA bit less than the training set, but not so bad.\nLet’s check the confusion matrix\n\ntbl = prop_table(new_y, new_pred_y)\n\n10×10 Named Matrix{Float64}\nDim1 ╲ Dim2 │    0     1     2     3     4     5     6     7     8     9\n────────────┼───────────────────────────────────────────────────────────\n0           │ 95.7   0.0   0.9   0.2   0.9   0.2   0.9   0.0   1.1   0.2\n1           │  0.5  95.4   1.1   0.4   0.9   0.5   0.2   1.1   0.0   0.0\n2           │  1.1   0.4  78.5   3.8   1.1   9.8   2.1   1.7   1.5   0.0\n3           │  0.0   1.0   5.8  85.0   0.0   3.8   0.2   3.4   0.8   0.0\n4           │  0.0   0.2   1.2   0.2  95.0   0.8   0.4   0.4   0.8   1.0\n5           │  0.4   0.9  14.9   4.2   2.2  73.5   0.9   1.1   1.5   0.4\n6           │  1.5   0.2   1.5   0.2   1.5   1.3  91.8   0.0   1.7   0.2\n7           │  0.2   3.1   3.7   4.7   6.6   1.4   0.2  79.3   0.0   0.8\n8           │  1.0   0.0   4.7   0.2   0.8   0.6   0.0   0.0  91.4   1.2\n9           │  0.2   0.2   1.0   0.2   5.8   0.4   0.6   2.1   4.6  85.0"
  },
  {
    "objectID": "digits.html#closing-remarks",
    "href": "digits.html#closing-remarks",
    "title": "6  Classifying hand-written digits",
    "section": "6.7 Closing remarks",
    "text": "6.7 Closing remarks\nEven though we used heavy machinery from topology, at the end our persistence images were vectors that indicated the birth and death of edges. Apart from that, the only machine learning algorithm we used was a simple dense neural network to fit these vectors to the correct labels in a non-linear way. State-of-art machine learning models on the MNIST dataset usually can get more than 99% of accuracy, but they use some complicated neural networks with many layers, and the output prediction are hard to explain. These methods, however, are not excludent of each other: we can use the persistence images (and any other vectorized output from TDA) together with other algorithms.\nA curious exercise to the reader is to check if a neural network with two parallel inputs - one for the digits images, followed by convolutional layers - other for the vector of persistence images, followed by dense layers can achieve a better result than the convolutional alone."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topological Data Analysis with Julia",
    "section": "",
    "text": "Preface\n\n“[O]f all the several ways of beginning a book which are now in practice throughout the known world, I am confident my own way of doing it is the best—I’m sure it is the most religious—for I begin with writing the first sentence—and trusting to Almighty God for the second.”\n— Laurence Sterne, in “The Life and Opinions of Tristram Shandy, Gentleman”\n\nWelcome!\nThis is the first draft of “Topological data analysis with Julia”.\nThe secret knowledge of Topological Data Analysis (TDA, for short) is spread in hundreds of papers and a few books. None, however, gives a consistent treatment of theory and examples with code. Code is essential to transform theory into real data analysis.\nThis book tries to fill this gap. In it, we will outline the main methods used to analyse data with topology, and try to give some non-trivial examples. Besides, it is a healthy way I found to practice Julia and study TDA again.\nThe readers who are afraid of Mathematics are urged to at least understand the intuitive notions of the definitions and results presented here. This is why I will give many informal descriptions of the ideas and objects before formalising them. Keep in mind, however, that Mathematics is the language that best describes abstractions and the use of logic, and you only can learn a language by using it.\nThis book will teach the basics of topology needed to understand TDA, and in the examples will give some directions on data analysis; but unfortunately we will not teach you Julia. For that, there are many excellent resources. See, for example:\n\nThink Julia\nJulia for Optimization and Learning\nData Science in Julia for Hackers"
  },
  {
    "objectID": "persistence.html#clustering-revisited",
    "href": "persistence.html#clustering-revisited",
    "title": "5  Persistent homology",
    "section": "5.1 Clustering revisited",
    "text": "5.1 Clustering revisited\nStaticians are smart people.\nDendrograms as a 0-dimensional persistence\nVietoris-Rips e primos"
  },
  {
    "objectID": "clustering.html#a-taste-of-topology",
    "href": "clustering.html#a-taste-of-topology",
    "title": "5  Clustering",
    "section": "5.2 A taste of topology",
    "text": "5.2 A taste of topology\nGiven a pointcloud \\(X\\), how topology can help us in creating clusters?\nThe ToMATo clustering algorithm needs the following ingredients:\n\na density function;\na proximity graph;\na parameter \\(\\tau\\) which can be seen as “how high can a slope be so we can consider it noise”.\n\n\n\n\nThe algorithm as stated in the original paper. Source: etc.\n\n\nLet’s understand this algorithm with an example.\n\n\n\n\nUltsch, Alfred, and Jörn Lötsch. 2020. “The Fundamental Clustering and Projection Suite (FCPS): A Dataset Collection to Test the Performance of Clustering and Data Projection Algorithms.” Data 5 (1). https://doi.org/10.3390/data5010013."
  },
  {
    "objectID": "clustering.html#datasets",
    "href": "clustering.html#datasets",
    "title": "5  Clustering",
    "section": "5.2 Datasets",
    "text": "5.2 Datasets\nThe datasets are taken from (Ultsch and Lötsch 2020)\n\n\n\n\nUltsch, Alfred, and Jörn Lötsch. 2020. “The Fundamental Clustering and Projection Suite (FCPS): A Dataset Collection to Test the Performance of Clustering and Data Projection Algorithms.” Data 5 (1). https://doi.org/10.3390/data5010013."
  },
  {
    "objectID": "clustering.html#tomato",
    "href": "clustering.html#tomato",
    "title": "5  Clustering",
    "section": "5.1 ToMATo",
    "text": "5.1 ToMATo\nGiven a pointcloud \\(X\\), how topology can help us in creating clusters?\nLet’s understand the ToMATo algorithm with an example and pretend we are actually inventing it. How fun!\nLet’s load some packages first\n\nusing ToMATo\nimport GeometricDatasets as gd\nusing AlgebraOfGraphics, \n# CairoMakie, \nGLMakie;\n\nSuppose \\(X\\) is the following dataset:\n\nX = hcat(randn(2, 800), randn(2, 800) .* 0.8 .+ (4, -4), randn(2, 100) .* 0.3 .- (2, -2))\ndf = (x1 = X[1, :], x2 = X[2, :])\nplt = data(df) * mapping(:x1, :x2)\ndraw(plt)\n\n\n\n\nWe can see two big clusters and possibly a small one.\nLet’s make a note-so-absurd supposition that our data is generated by some normal distribuitions (in our case, it really is; in general this can be false). Then, intuitively, the mean of each distribution is the point with the highest density, ie, with more points close to it, because it is in the center of the distribution.\n\n5.1.1 Density\nWe can estimate the density of a dataset as follows\n\nds = gd.density_estimation(X, h = 0.5)\n\ndf = (x1 = X[1, :], x2 = X[2, :], ds = ds)\nplt = data(df) * mapping(:x1, :x2, color = :ds)\ndraw(plt)\n\n\n\n\nThe precise definition of the density of a point \\(x \\in X\\) is\n\\[\n\\text{dens}(x) = \\dfrac{1}{|X|} \\cdot \\sum_{y \\in X} \\text{exp} \\left(-\\dfrac{d(x, y)^2}{h^2} \\right)\n\\]\nwhere \\(\\text{exp}(x) = e^x\\). Informally, we want that points close (ie. small distance) to many other points have a high density; this is way we calculate the distance from \\(x\\) to \\(y\\) and put it inside an exponential with a minus sign. We then calculate the mean of all these values.\nNow we put this density estimation on another axis and plot it\n\naxis = (type = Axis3, width = 800, height = 450)\ndf = (x1 = X[1, :], x2 = X[2, :], ds = ds)\nplt = data(df) * mapping(:x1, :x2, :ds, color = :ds)\ndraw(plt; axis = axis)\n\n\n\n\nGood!\n\n\n5.1.2 Climbing mount topology\nThe idea now is that given a point \\(x\\):\n\nif \\(x\\) is the highest point in its corresponding mountain, then it is a new cluster;\notherwise, we will we seek for the highest neighbor of \\(x\\), say \\(x'\\) and say that the cluster of \\(x\\) is the same of \\(x'\\).\n\nTo do that, we need to define a notion of “neighborhood” in this dataset. The easiest way is to define a graph whose vertex set is \\(X\\) and edges connect neighbor points. Fortunately, the ToMATo package has a function that does exactly that. You are welcome!\n\ng = proximity_graph(X, 0.2, max_k_ball = 4, k_nn = 3, min_k_ball = 2)\n\n{1700, 4793} undirected simple Int64 graph\n\n\nThe graph \\(g\\) above is constructed as follows: given \\(x \\in X\\), we create a ball with radius \\(0.2\\) around \\(X\\) and do the following:\n\nAre there less than 2 points in the ball? If yes, then we connect \\(x\\) to its 2 closest points; if no, we connect \\(x\\) to its 6 (at maximum) closest points in the ball. In short: if the ball has not the amount of points we stipulated, then we use knn search.\n\nThese numbers obviously are arbitrary and can be changed at will.\nLet’s see the result of our algorithm:\n\nX2 = vcat(X, ds')\nclusters, _ = tomato(X, g, ds, 0)\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nLook how many clusters! This is obviously wrong.\n\n\n5.1.3 The Comedy of Errors\nThis tragedy happened because we did not take into account the “false peaks”: peaks that are just a little slump (!!!) and not a real peak. To merge these false-peaks into the big ones, we need to add the following step:\nLet \\(\\tau\\) be a number that denotes how small a slump must be to be merged (we will show how to calculate \\(\\tau\\) below). Given \\(x \\in X\\), let \\(N\\) be the set of its neighbors higher than \\(x\\). Denote by \\(x_max\\) the highest point in \\(N\\), and \\(c_max\\) its cluster. Now, for each \\(y \\in N\\), ask the following:\n\nIs the difference of heights of \\(x\\) and \\(y\\) less than \\(\\tau\\)? If yes, we merge the cluster of \\(y\\) with the cluster of \\(x_max\\). Otherwise, do nothing.\n\n\nτ = 0.02\nclusters, _ = tomato(X, g, ds, τ, max_cluster_height = τ)\n\nX2 = vcat(X, ds')\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nNow we got something!\nBut how did we calculate this magic \\(\\tau\\)? Should we just go on guessing real numbers? I hope not!\nWe usually run the ToMATo algorithm twice. The first time, we put \\(\\tau = \\inf\\) and see how the montains of \\(X\\) merged: we plot the time each one of these mountains survived.\n\n_, births_and_deaths = tomato(X, g, ds, Inf)\nplot_births_and_deaths(births_and_deaths)\n\n\n\n\nChoosing a bigger \\(\\tau\\) (say 0.04) will also merge the small clusters on the left\n\nτ = 0.04\nclusters, _ = tomato(X, g, ds, τ, max_cluster_height = τ)\n\nX2 = vcat(X, ds')\nfig, ax, plt = graph_plot(X2, g, clusters .|&gt; string)\nfig\n\n┌ Warning: Axis got passed, but also axis attributes. Ignoring axis attributes (type = Axis3, width = 600, height = 600).\n└ @ AlgebraOfGraphics ~/.julia/packages/AlgebraOfGraphics/yhdjr/src/draw.jl:19\n\n\n\n\n\nAt the end of the day you will still need to define how bigger a slump you can accept before merging it.\n\n\n5.1.4 The formal algorithm\nAfter all this talk, maybe the original algorithm can be better understood\n\nFor the curious reader, the math above is easily translated to Julia\n\nfunction tomato(\n    X::PointCloud, g::Graph, ds::Vector{&lt;:Real}, τ::Real = Inf;\n    max_cluster_height::Real = 0\n    )\n    sorted_ids = sortperm(ds, rev = true)\n    clusters = zeros(Int64, size(X)[2])\n    births_and_deaths = Dict{Int64, Vector{&lt;: Real}}()\n\n    for i ∈ sorted_ids\n        N = neighbors(g, i) |&gt; copy\n        filter!(x -&gt; ds[x] &gt; ds[i], N)\n    \n        # if there is no upper-neighbor\n        if length(N) == 0\n            clusters[i] = i\n            births_and_deaths[i] = [ds[i], Inf]\n            continue\n        end\n    \n        c_max = clusters[argmax(x -&gt; ds[x], N)]\n        clusters[i] = c_max\n        \n        for j ∈ N\n            c_j = clusters[j]\n    \n            # if the clusters are equal, skip\n            c_max == c_j && continue\n    \n            # if c_j has no cluster, put j on c_max\n            if c_j == 0\n                update_cluster!(clusters, j, c_max)\n                continue\n            end\n    \n            # If the lowest of them is just a bit below the current height ds[i],        \n            # we merge the clusters        \n            if min(ds[c_max], ds[c_j]) &lt; ds[i] + τ\n                from, to = sort([c_max, c_j], by = x -&gt; ds[x])\n                births_and_deaths[from][2] = ds[i]\n                replace!(clusters, from =&gt; to)\n            end\n        end    \n    end\n\n    sorted_clusters = sort(clusters |&gt; unique, by = x -&gt; ds[x], rev = true)\n\n    cluster_dict = \n        map(sorted_clusters) do cl\n\n            if  (ds[cl] &lt; max_cluster_height)\n                true_number = 0\n            else\n                true_number = findfirst(x -&gt; x == cl, sorted_clusters)\n            end\n\n            Dict(cl =&gt; true_number)\n        end\n    \n    cluster_dict = merge(cluster_dict...)\n\n    final_clusters = replace(clusters, cluster_dict...)\n\n    return final_clusters, births_and_deaths\n    \nend"
  }
]